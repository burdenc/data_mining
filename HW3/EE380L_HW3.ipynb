{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 80</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, March 20th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Gradient Descent (5+12+3 = 20pts)\n",
    "\n",
    "In this question you will implement vanilla SGD and 2 adaptive gradient update techniques called Adagrad and Adadelta. In addition, you will also implement ridge regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: $$ y = w_0 + w_1*x_1 + w_2*x_1*x_1 + w_3*x_2*x_2 + w_4*x_2 $$ Hint: start from the cost function (Assume sum of squared error). If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for an SGD solution to the non-linear model $$ y = w_0 + w_1*x_1 + w_2*x_1*x_1 + w_3*x_2*x_2 + w_4*x_2 $$ Try to format similarly to scikit-learn's models. Your Python class should take as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y and a choice of update_rule as 'SGD', 'adagrad' or 'adadelta' (Notes on implementation below). The _predict_ method takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 20 epochs (this may have to increase for adadelta), using learning rates: [.0001, .001, .01, 0.1, 1, 10, 100] and regularization constants in the range: [0,10,100] . Plot MSE and the $w$ parameters as a function of epoch count (20 epochs) for the best combination of learning_rate and regularization for both SGD, Adagrad and Adadelta. ie you should have a plot of MSE and parameter updates for SGD, adagrad and adadelta. Report the MSE at the end of 10 epochs for all 3.\n",
    "3. Based on the experiments, answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Among SGD and Adagrad, which one allows for larger initial setting of the learning_rate? Why?\n",
    "2. Mention one benefit and one drawback of Adadelta over Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Adagrad/Adadelta\n",
    "Adagrad (http://ruder.io/optimizing-gradient-descent/) differs from vanilla SGD in that the learning rate of each weight changes over updates. A cache is maintained that holds the sum of squares of all gradients upto the current update. The learning_rate is divided by the cache, resulting in a different learning rate for each weight. A consequence of this update rule is that weights that have already seen large gradients (made large jumps) make smaller updates in subsequent iterations.\n",
    "Specifically, the steps can be listed as below:\n",
    "1. cache = cache + (gradients^2)\n",
    "2. weights = weights + ((learning_rate)/sqrt(cache+1e-6))*gradients\n",
    "\n",
    "The key difference between Adadelta and Adagrad is that the former uses a weighted sum for each update of the cache. Also, you will not need to use a learning rate to make Adadelta work. In addition to the link above, the paper on Adadelta is a great resource to implement it correctly (https://arxiv.org/pdf/1212.5701.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Notes on Training with Gradient Descent\n",
    "1. Compute error: This consists of a prediction error and a regularization term. From an implementation perspective, this is a function that takes as input the truth, prediction and regularization hyperparameter and returns an error\n",
    "2. Compute gradients: Take a derivative of the error in terms of the weights. This can be modelled as a function that takes as input the error and features and returns the gradients for each weight\n",
    "3. Update weights: Weight updates can be done using vanilla SGD or adaptive techniques. The update function takes as inputs the gradient and hyperparameters and returns the new weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "<img src=\"1_math.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "rate:  0.0001  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  14.196912060031895  Epoch_num:  0\n",
      "MSE:  14.097845611437723  Epoch_num:  1\n",
      "MSE:  14.055669849642282  Epoch_num:  2\n",
      "MSE:  14.023366905728947  Epoch_num:  3\n",
      "MSE:  13.996177927534928  Epoch_num:  4\n",
      "MSE:  13.972258022392039  Epoch_num:  5\n",
      "MSE:  13.950660631175019  Epoch_num:  6\n",
      "MSE:  13.930823317481211  Epoch_num:  7\n",
      "MSE:  13.91237956323974  Epoch_num:  8\n",
      "MSE:  13.895074754433258  Epoch_num:  9\n",
      "MSE:  13.878723491894721  Epoch_num:  10\n",
      "MSE:  13.863185804683683  Epoch_num:  11\n",
      "MSE:  13.848352943379188  Epoch_num:  12\n",
      "MSE:  13.83413842143388  Epoch_num:  13\n",
      "MSE:  13.820472113304564  Epoch_num:  14\n",
      "MSE:  13.807296223699234  Epoch_num:  15\n",
      "MSE:  13.794562450256477  Epoch_num:  16\n",
      "MSE:  13.782229934250923  Epoch_num:  17\n",
      "MSE:  13.770263747269869  Epoch_num:  18\n",
      "MSE:  13.758633751883059  Epoch_num:  19\n",
      "MSE:  13.747313729184956  Epoch_num:  20\n",
      "MSE:  13.747313729184956  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  13.940851649607538  Epoch_num:  0\n",
      "MSE:  1.3650247960227657  Epoch_num:  1\n",
      "MSE:  0.5810610738446178  Epoch_num:  2\n",
      "MSE:  0.22948022541927773  Epoch_num:  3\n",
      "MSE:  0.12787001918432947  Epoch_num:  4\n",
      "MSE:  0.10707944677220357  Epoch_num:  5\n",
      "MSE:  0.09997562176248131  Epoch_num:  6\n",
      "MSE:  0.09474023067209006  Epoch_num:  7\n",
      "MSE:  0.08997042681578289  Epoch_num:  8\n",
      "MSE:  0.08542366387543616  Epoch_num:  9\n",
      "MSE:  0.08103280064259664  Epoch_num:  10\n",
      "MSE:  0.07677470248074132  Epoch_num:  11\n",
      "MSE:  0.07264089750374013  Epoch_num:  12\n",
      "MSE:  0.06862821333828699  Epoch_num:  13\n",
      "MSE:  0.06473538338694755  Epoch_num:  14\n",
      "MSE:  0.060961799580049214  Epoch_num:  15\n",
      "MSE:  0.05730706407308378  Epoch_num:  16\n",
      "MSE:  0.05377083263243932  Epoch_num:  17\n",
      "MSE:  0.05035276097968396  Epoch_num:  18\n",
      "MSE:  0.04705248617178461  Epoch_num:  19\n",
      "MSE:  0.04386961949813648  Epoch_num:  20\n",
      "MSE:  0.04386961949813648  Epoch_num:  21\n",
      "rate:  0.001  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  16.14710907480816  Epoch_num:  0\n",
      "MSE:  15.097510750134619  Epoch_num:  1\n",
      "MSE:  14.666220684451885  Epoch_num:  2\n",
      "MSE:  14.34210140860408  Epoch_num:  3\n",
      "MSE:  14.073459198858423  Epoch_num:  4\n",
      "MSE:  13.840257963420802  Epoch_num:  5\n",
      "MSE:  13.632219833049664  Epoch_num:  6\n",
      "MSE:  13.443238977120156  Epoch_num:  7\n",
      "MSE:  13.26933721937407  Epoch_num:  8\n",
      "MSE:  13.107751507994548  Epoch_num:  9\n",
      "MSE:  12.956470282374603  Epoch_num:  10\n",
      "MSE:  12.813975156376971  Epoch_num:  11\n",
      "MSE:  12.67908664995519  Epoch_num:  12\n",
      "MSE:  12.550866916682757  Epoch_num:  13\n",
      "MSE:  12.428555666848064  Epoch_num:  14\n",
      "MSE:  12.311526408931018  Epoch_num:  15\n",
      "MSE:  12.199255649682094  Epoch_num:  16\n",
      "MSE:  12.091300650249366  Epoch_num:  17\n",
      "MSE:  11.98728300127944  Epoch_num:  18\n",
      "MSE:  11.886876258158965  Epoch_num:  19\n",
      "MSE:  11.789796473292899  Epoch_num:  20\n",
      "MSE:  11.789796473292899  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  15.647749880681321  Epoch_num:  0\n",
      "MSE:  1.5355952823326398  Epoch_num:  1\n",
      "MSE:  0.7370932467749519  Epoch_num:  2\n",
      "MSE:  0.32259278552248793  Epoch_num:  3\n",
      "MSE:  0.15724003349029822  Epoch_num:  4\n",
      "MSE:  0.11256071986989019  Epoch_num:  5\n",
      "MSE:  0.10089769291359536  Epoch_num:  6\n",
      "MSE:  0.0948735875521127  Epoch_num:  7\n",
      "MSE:  0.08994767091554118  Epoch_num:  8\n",
      "MSE:  0.08538115277074788  Epoch_num:  9\n",
      "MSE:  0.0810060321826202  Epoch_num:  10\n",
      "MSE:  0.07677262472375015  Epoch_num:  11\n",
      "MSE:  0.07266435457092248  Epoch_num:  12\n",
      "MSE:  0.068675459125546  Epoch_num:  13\n",
      "MSE:  0.06480386820572241  Epoch_num:  14\n",
      "MSE:  0.06104875953883218  Epoch_num:  15\n",
      "MSE:  0.05740971520813436  Epoch_num:  16\n",
      "MSE:  0.05388643635281472  Epoch_num:  17\n",
      "MSE:  0.05047864916231097  Epoch_num:  18\n",
      "MSE:  0.047186074504325486  Epoch_num:  19\n",
      "MSE:  0.0440084180508683  Epoch_num:  20\n",
      "MSE:  0.0440084180508683  Epoch_num:  21\n",
      "rate:  0.01  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  14.984620187261472  Epoch_num:  0\n",
      "MSE:  7.254813549946663  Epoch_num:  1\n",
      "MSE:  5.308562176697218  Epoch_num:  2\n",
      "MSE:  4.235765353814374  Epoch_num:  3\n",
      "MSE:  3.5580327036252934  Epoch_num:  4\n",
      "MSE:  3.097756189287181  Epoch_num:  5\n",
      "MSE:  2.769091487765153  Epoch_num:  6\n",
      "MSE:  2.5248392493577168  Epoch_num:  7\n",
      "MSE:  2.3370078764849214  Epoch_num:  8\n",
      "MSE:  2.1881438111769986  Epoch_num:  9\n",
      "MSE:  2.066957450065241  Epoch_num:  10\n",
      "MSE:  1.9659297208711137  Epoch_num:  11\n",
      "MSE:  1.8799271092196952  Epoch_num:  12\n",
      "MSE:  1.8053669863421495  Epoch_num:  13\n",
      "MSE:  1.7396985523264539  Epoch_num:  14\n",
      "MSE:  1.681071666688157  Epoch_num:  15\n",
      "MSE:  1.6281208619890917  Epoch_num:  16\n",
      "MSE:  1.5798217090137119  Epoch_num:  17\n",
      "MSE:  1.5353935948181165  Epoch_num:  18\n",
      "MSE:  1.4942328371986864  Epoch_num:  19\n",
      "MSE:  1.4558659684802187  Epoch_num:  20\n",
      "MSE:  1.4558659684802187  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  20.227868372520717  Epoch_num:  0\n",
      "MSE:  1.3365627175529866  Epoch_num:  1\n",
      "MSE:  0.6158034194784997  Epoch_num:  2\n",
      "MSE:  0.27440135751262784  Epoch_num:  3\n",
      "MSE:  0.15385545413438984  Epoch_num:  4\n",
      "MSE:  0.12308869148324288  Epoch_num:  5\n",
      "MSE:  0.11329405529499895  Epoch_num:  6\n",
      "MSE:  0.10719431940370999  Epoch_num:  7\n",
      "MSE:  0.10196102157462936  Epoch_num:  8\n",
      "MSE:  0.09705876482404621  Epoch_num:  9\n",
      "MSE:  0.0923480326700144  Epoch_num:  10\n",
      "MSE:  0.0877832748128026  Epoch_num:  11\n",
      "MSE:  0.08334794473539418  Epoch_num:  12\n",
      "MSE:  0.07903579171085044  Epoch_num:  13\n",
      "MSE:  0.07484438690932525  Epoch_num:  14\n",
      "MSE:  0.07077271243998498  Epoch_num:  15\n",
      "MSE:  0.06682025383852983  Epoch_num:  16\n",
      "MSE:  0.06298666343619877  Epoch_num:  17\n",
      "MSE:  0.05927163803994921  Epoch_num:  18\n",
      "MSE:  0.05567487504381596  Epoch_num:  19\n",
      "MSE:  0.05219605637233477  Epoch_num:  20\n",
      "MSE:  0.05219605637233477  Epoch_num:  21\n",
      "rate:  0.1  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  16.56607182416177  Epoch_num:  0\n",
      "MSE:  0.5697518263985398  Epoch_num:  1\n",
      "MSE:  0.27566979263246993  Epoch_num:  2\n",
      "MSE:  0.18301449062705993  Epoch_num:  3\n",
      "MSE:  0.1436409728610516  Epoch_num:  4\n",
      "MSE:  0.12381530623209851  Epoch_num:  5\n",
      "MSE:  0.11265368933296875  Epoch_num:  6\n",
      "MSE:  0.10577395688577226  Epoch_num:  7\n",
      "MSE:  0.1011523883666018  Epoch_num:  8\n",
      "MSE:  0.09777588862776912  Epoch_num:  9\n",
      "MSE:  0.09511207838066822  Epoch_num:  10\n",
      "MSE:  0.09287209999877004  Epoch_num:  11\n",
      "MSE:  0.09089539669219862  Epoch_num:  12\n",
      "MSE:  0.08909082614448038  Epoch_num:  13\n",
      "MSE:  0.08740555697702498  Epoch_num:  14\n",
      "MSE:  0.08580825530004961  Epoch_num:  15\n",
      "MSE:  0.08427983307365242  Epoch_num:  16\n",
      "MSE:  0.08280828163996011  Epoch_num:  17\n",
      "MSE:  0.08138574854071703  Epoch_num:  18\n",
      "MSE:  0.08000686323675779  Epoch_num:  19\n",
      "MSE:  0.07866776664256116  Epoch_num:  20\n",
      "MSE:  0.07866776664256116  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  10.939291076029269  Epoch_num:  0\n",
      "MSE:  1.602532249652283  Epoch_num:  1\n",
      "MSE:  0.7602886999165658  Epoch_num:  2\n",
      "MSE:  0.32971082202348295  Epoch_num:  3\n",
      "MSE:  0.17009872334635043  Epoch_num:  4\n",
      "MSE:  0.12886797069246125  Epoch_num:  5\n",
      "MSE:  0.11744641287948135  Epoch_num:  6\n",
      "MSE:  0.11103196465141882  Epoch_num:  7\n",
      "MSE:  0.10565534952102257  Epoch_num:  8\n",
      "MSE:  0.10064669349938708  Epoch_num:  9\n",
      "MSE:  0.09584204774654403  Epoch_num:  10\n",
      "MSE:  0.09118830216081639  Epoch_num:  11\n",
      "MSE:  0.08666590789304894  Epoch_num:  12\n",
      "MSE:  0.08226736289211545  Epoch_num:  13\n",
      "MSE:  0.07798972977871475  Epoch_num:  14\n",
      "MSE:  0.07383179618572012  Epoch_num:  15\n",
      "MSE:  0.06979298233222413  Epoch_num:  16\n",
      "MSE:  0.06587292704610946  Epoch_num:  17\n",
      "MSE:  0.062071334360959116  Epoch_num:  18\n",
      "MSE:  0.05838791765406201  Epoch_num:  19\n",
      "MSE:  0.054822379191602974  Epoch_num:  20\n",
      "MSE:  0.054822379191602974  Epoch_num:  21\n",
      "rate:  1  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  10.31681005784572  Epoch_num:  0\n",
      "MSE:  0.04353480650607658  Epoch_num:  1\n",
      "MSE:  0.016682543956241053  Epoch_num:  2\n",
      "MSE:  0.006887530836889811  Epoch_num:  3\n",
      "MSE:  0.002923878972844362  Epoch_num:  4\n",
      "MSE:  0.00125873340182576  Epoch_num:  5\n",
      "MSE:  0.0005475859980363791  Epoch_num:  6\n",
      "MSE:  0.0002411243229695585  Epoch_num:  7\n",
      "MSE:  0.00010815749472841671  Epoch_num:  8\n",
      "MSE:  5.0055796964549036e-05  Epoch_num:  9\n",
      "MSE:  2.4439419316492015e-05  Epoch_num:  10\n",
      "MSE:  1.3007939627079964e-05  Epoch_num:  11\n",
      "MSE:  7.821459497920872e-06  Epoch_num:  12\n",
      "MSE:  5.415530368544225e-06  Epoch_num:  13\n",
      "MSE:  4.2669795144640385e-06  Epoch_num:  14\n",
      "MSE:  3.6990620108507653e-06  Epoch_num:  15\n",
      "MSE:  3.4066964667093884e-06  Epoch_num:  16\n",
      "MSE:  3.2496047555347486e-06  Epoch_num:  17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  3.1615884576863765e-06  Epoch_num:  18\n",
      "MSE:  3.110374135741456e-06  Epoch_num:  19\n",
      "MSE:  3.079612340056622e-06  Epoch_num:  20\n",
      "MSE:  3.079612340056622e-06  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  18.477215899140322  Epoch_num:  0\n",
      "MSE:  1.8131622906501035  Epoch_num:  1\n",
      "MSE:  0.8740415053953879  Epoch_num:  2\n",
      "MSE:  0.3772159406753261  Epoch_num:  3\n",
      "MSE:  0.17358525997534643  Epoch_num:  4\n",
      "MSE:  0.11786499509465659  Epoch_num:  5\n",
      "MSE:  0.10422936905307205  Epoch_num:  6\n",
      "MSE:  0.09778008930896705  Epoch_num:  7\n",
      "MSE:  0.09266244558253406  Epoch_num:  8\n",
      "MSE:  0.08795598674976106  Epoch_num:  9\n",
      "MSE:  0.08345923773394512  Epoch_num:  10\n",
      "MSE:  0.07911358795133928  Epoch_num:  11\n",
      "MSE:  0.07489924513027353  Epoch_num:  12\n",
      "MSE:  0.070809124894154  Epoch_num:  13\n",
      "MSE:  0.06684059271134811  Epoch_num:  14\n",
      "MSE:  0.0629925720047638  Epoch_num:  15\n",
      "MSE:  0.05926451090177591  Epoch_num:  16\n",
      "MSE:  0.05565601805843688  Epoch_num:  17\n",
      "MSE:  0.05216673700571074  Epoch_num:  18\n",
      "MSE:  0.048796303220207234  Epoch_num:  19\n",
      "MSE:  0.04554432894342792  Epoch_num:  20\n",
      "MSE:  0.04554432894342792  Epoch_num:  21\n",
      "rate:  10  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  18.488210559496853  Epoch_num:  0\n",
      "MSE:  0.00641617797002704  Epoch_num:  1\n",
      "MSE:  0.00023648621927440633  Epoch_num:  2\n",
      "MSE:  2.911468918568473e-05  Epoch_num:  3\n",
      "MSE:  1.607343727101803e-05  Epoch_num:  4\n",
      "MSE:  1.4488214201309624e-05  Epoch_num:  5\n",
      "MSE:  1.423566459202771e-05  Epoch_num:  6\n",
      "MSE:  1.4192196744775759e-05  Epoch_num:  7\n",
      "MSE:  1.4184428954789993e-05  Epoch_num:  8\n",
      "MSE:  1.418295442798469e-05  Epoch_num:  9\n",
      "MSE:  1.418261221281667e-05  Epoch_num:  10\n",
      "MSE:  1.4182477189392585e-05  Epoch_num:  11\n",
      "MSE:  1.4182380634457038e-05  Epoch_num:  12\n",
      "MSE:  1.418229131245518e-05  Epoch_num:  13\n",
      "MSE:  1.4182203366061744e-05  Epoch_num:  14\n",
      "MSE:  1.4182115685010561e-05  Epoch_num:  15\n",
      "MSE:  1.418202805696046e-05  Epoch_num:  16\n",
      "MSE:  1.418194044101468e-05  Epoch_num:  17\n",
      "MSE:  1.4181852829236393e-05  Epoch_num:  18\n",
      "MSE:  1.4181765220094848e-05  Epoch_num:  19\n",
      "MSE:  1.4181677613285924e-05  Epoch_num:  20\n",
      "MSE:  1.4181677613285924e-05  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  11.321746717842567  Epoch_num:  0\n",
      "MSE:  1.0822718077115197  Epoch_num:  1\n",
      "MSE:  0.4763276845974175  Epoch_num:  2\n",
      "MSE:  0.2069669099214852  Epoch_num:  3\n",
      "MSE:  0.12518243617428057  Epoch_num:  4\n",
      "MSE:  0.10611806765082295  Epoch_num:  5\n",
      "MSE:  0.0988029663287298  Epoch_num:  6\n",
      "MSE:  0.09348967591488276  Epoch_num:  7\n",
      "MSE:  0.08871052175571827  Epoch_num:  8\n",
      "MSE:  0.084172402870353  Epoch_num:  9\n",
      "MSE:  0.07979552029282251  Epoch_num:  10\n",
      "MSE:  0.07555376688291236  Epoch_num:  11\n",
      "MSE:  0.07143791500234813  Epoch_num:  12\n",
      "MSE:  0.06744457598910297  Epoch_num:  13\n",
      "MSE:  0.06357241056375025  Epoch_num:  14\n",
      "MSE:  0.05982077801861068  Epoch_num:  15\n",
      "MSE:  0.05618925811935128  Epoch_num:  16\n",
      "MSE:  0.05267748530903298  Epoch_num:  17\n",
      "MSE:  0.04928509200348967  Epoch_num:  18\n",
      "MSE:  0.046011688771936565  Epoch_num:  19\n",
      "MSE:  0.04285685657602295  Epoch_num:  20\n",
      "MSE:  0.04285685657602295  Epoch_num:  21\n",
      "rate:  100  reg:  0\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  15.088956781384802  Epoch_num:  0\n",
      "MSE:  0.006110917123394872  Epoch_num:  1\n",
      "MSE:  0.00021322816461379995  Epoch_num:  2\n",
      "MSE:  2.6533848495475214e-05  Epoch_num:  3\n",
      "MSE:  1.5039723071744552e-05  Epoch_num:  4\n",
      "MSE:  1.3656357781524264e-05  Epoch_num:  5\n",
      "MSE:  1.343491846663234e-05  Epoch_num:  6\n",
      "MSE:  1.3395682936473999e-05  Epoch_num:  7\n",
      "MSE:  1.3388333991628812e-05  Epoch_num:  8\n",
      "MSE:  1.3386898944852058e-05  Epoch_num:  9\n",
      "MSE:  1.3386608932432947e-05  Epoch_num:  10\n",
      "MSE:  1.338654836120735e-05  Epoch_num:  11\n",
      "MSE:  1.3386535011257294e-05  Epoch_num:  12\n",
      "MSE:  1.3386531576092267e-05  Epoch_num:  13\n",
      "MSE:  1.338653025286608e-05  Epoch_num:  14\n",
      "MSE:  1.338652938404662e-05  Epoch_num:  15\n",
      "MSE:  1.3386528613687513e-05  Epoch_num:  16\n",
      "MSE:  1.3386527864763097e-05  Epoch_num:  17\n",
      "MSE:  1.3386527120507127e-05  Epoch_num:  18\n",
      "MSE:  1.3386526377278924e-05  Epoch_num:  19\n",
      "MSE:  1.3386525634272038e-05  Epoch_num:  20\n",
      "MSE:  1.3386525634272038e-05  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  16.834321807150207  Epoch_num:  0\n",
      "MSE:  1.7113409798497072  Epoch_num:  1\n",
      "MSE:  0.7987936262062288  Epoch_num:  2\n",
      "MSE:  0.31873620612791403  Epoch_num:  3\n",
      "MSE:  0.13337673546803283  Epoch_num:  4\n",
      "MSE:  0.08946077186064337  Epoch_num:  5\n",
      "MSE:  0.07969796858122336  Epoch_num:  6\n",
      "MSE:  0.0746028138219565  Epoch_num:  7\n",
      "MSE:  0.07031337473838002  Epoch_num:  8\n",
      "MSE:  0.06629723511348436  Epoch_num:  9\n",
      "MSE:  0.06244410261229312  Epoch_num:  10\n",
      "MSE:  0.058723898836608945  Epoch_num:  11\n",
      "MSE:  0.05512739073677702  Epoch_num:  12\n",
      "MSE:  0.05165154605752513  Epoch_num:  13\n",
      "MSE:  0.048295207289860705  Epoch_num:  14\n",
      "MSE:  0.045057755342136796  Epoch_num:  15\n",
      "MSE:  0.041938702249514036  Epoch_num:  16\n",
      "MSE:  0.03893757044448941  Epoch_num:  17\n",
      "MSE:  0.03605385757541264  Epoch_num:  18\n",
      "MSE:  0.03328702603699873  Epoch_num:  19\n",
      "MSE:  0.030636499688280548  Epoch_num:  20\n",
      "MSE:  0.030636499688280548  Epoch_num:  21\n",
      "rate:  0.0001  reg:  10\n",
      "sgd\n",
      "MSE:  13.681112871298488  Epoch_num:  0\n",
      "MSE:  6.181008877240429  Epoch_num:  1\n",
      "MSE:  4.911415547261024  Epoch_num:  2\n",
      "MSE:  4.6368298749717285  Epoch_num:  3\n",
      "MSE:  4.572587033673208  Epoch_num:  4\n",
      "MSE:  4.557527837874548  Epoch_num:  5\n",
      "MSE:  4.554121895314049  Epoch_num:  6\n",
      "MSE:  4.553408896072443  Epoch_num:  7\n",
      "MSE:  4.553283494915896  Epoch_num:  8\n",
      "MSE:  4.553271963271574  Epoch_num:  9\n",
      "MSE:  4.553276414472905  Epoch_num:  10\n",
      "MSE:  4.553280216346779  Epoch_num:  11\n",
      "MSE:  4.553282146492211  Epoch_num:  12\n",
      "MSE:  4.553282984363262  Epoch_num:  13\n",
      "MSE:  4.553283322314835  Epoch_num:  14\n",
      "MSE:  4.553283453159939  Epoch_num:  15\n",
      "MSE:  4.5532835025715395  Epoch_num:  16\n",
      "MSE:  4.5532835209343485  Epoch_num:  17\n",
      "MSE:  4.553283527686202  Epoch_num:  18\n",
      "MSE:  4.553283530150909  Epoch_num:  19\n",
      "MSE:  4.553283531046153  Epoch_num:  20\n",
      "MSE:  4.553283531046153  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  15.647420845359344  Epoch_num:  0\n",
      "MSE:  15.527603170117226  Epoch_num:  1\n",
      "MSE:  15.476852290214865  Epoch_num:  2\n",
      "MSE:  15.438018663320182  Epoch_num:  3\n",
      "MSE:  15.405355564898157  Epoch_num:  4\n",
      "MSE:  15.376636234090112  Epoch_num:  5\n",
      "MSE:  15.350718459707164  Epoch_num:  6\n",
      "MSE:  15.326923648670636  Epoch_num:  7\n",
      "MSE:  15.30480963117861  Epoch_num:  8\n",
      "MSE:  15.284069260882534  Epoch_num:  9\n",
      "MSE:  15.264478898779466  Epoch_num:  10\n",
      "MSE:  15.245869711097384  Epoch_num:  11\n",
      "MSE:  15.228110528123578  Epoch_num:  12\n",
      "MSE:  15.211097035783517  Epoch_num:  13\n",
      "MSE:  15.194744655522467  Epoch_num:  14\n",
      "MSE:  15.178983681742722  Epoch_num:  15\n",
      "MSE:  15.163755859081286  Epoch_num:  16\n",
      "MSE:  15.149011910377837  Epoch_num:  17\n",
      "MSE:  15.134709711225792  Epoch_num:  18\n",
      "MSE:  15.120812915685985  Epoch_num:  19\n",
      "MSE:  15.107289903930367  Epoch_num:  20\n",
      "MSE:  15.107289903930367  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  11.188222106118472  Epoch_num:  0\n",
      "MSE:  4.56769770903356  Epoch_num:  1\n",
      "MSE:  4.567674363060183  Epoch_num:  2\n",
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  0.001  reg:  10\n",
      "sgd\n",
      "MSE:  18.4208350047419  Epoch_num:  0\n",
      "MSE:  4.565919921903794  Epoch_num:  1\n",
      "MSE:  4.565903576248075  Epoch_num:  2\n",
      "MSE:  4.565903576011951  Epoch_num:  3\n",
      "MSE:  4.565903576011944  Epoch_num:  4\n",
      "MSE:  4.565903576011944  Epoch_num:  5\n",
      "MSE:  4.565903576011944  Epoch_num:  6\n",
      "MSE:  4.565903576011944  Epoch_num:  7\n",
      "MSE:  4.565903576011944  Epoch_num:  8\n",
      "MSE:  4.565903576011944  Epoch_num:  9\n",
      "MSE:  4.565903576011944  Epoch_num:  10\n",
      "MSE:  4.565903576011944  Epoch_num:  11\n",
      "MSE:  4.565903576011944  Epoch_num:  12\n",
      "MSE:  4.565903576011944  Epoch_num:  13\n",
      "MSE:  4.565903576011944  Epoch_num:  14\n",
      "MSE:  4.565903576011944  Epoch_num:  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4.565903576011944  Epoch_num:  16\n",
      "MSE:  4.565903576011944  Epoch_num:  17\n",
      "MSE:  4.565903576011944  Epoch_num:  18\n",
      "MSE:  4.565903576011944  Epoch_num:  19\n",
      "MSE:  4.565903576011944  Epoch_num:  20\n",
      "MSE:  4.565903576011944  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  13.3907930410037  Epoch_num:  0\n",
      "MSE:  12.387513556600647  Epoch_num:  1\n",
      "MSE:  11.988608643655157  Epoch_num:  2\n",
      "MSE:  11.69405473405995  Epoch_num:  3\n",
      "MSE:  11.453454975911258  Epoch_num:  4\n",
      "MSE:  11.247282744248455  Epoch_num:  5\n",
      "MSE:  11.065523531893941  Epoch_num:  6\n",
      "MSE:  10.902228801976444  Epoch_num:  7\n",
      "MSE:  10.75352326223876  Epoch_num:  8\n",
      "MSE:  10.616715525981622  Epoch_num:  9\n",
      "MSE:  10.489846315969535  Epoch_num:  10\n",
      "MSE:  10.371436768866726  Epoch_num:  11\n",
      "MSE:  10.260338134898454  Epoch_num:  12\n",
      "MSE:  10.155637015336168  Epoch_num:  13\n",
      "MSE:  10.056592943833143  Epoch_num:  14\n",
      "MSE:  9.96259576341165  Epoch_num:  15\n",
      "MSE:  9.873135627823961  Epoch_num:  16\n",
      "MSE:  9.787781337755188  Epoch_num:  17\n",
      "MSE:  9.706164345213892  Epoch_num:  18\n",
      "MSE:  9.627966712621612  Epoch_num:  19\n",
      "MSE:  9.552911893542797  Epoch_num:  20\n",
      "MSE:  9.552911893542797  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  12.957945355743524  Epoch_num:  0\n",
      "MSE:  4.567669122161882  Epoch_num:  1\n",
      "MSE:  4.567674362995391  Epoch_num:  2\n",
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  0.01  reg:  10\n",
      "sgd\n",
      "MSE:  17.0875918505538  Epoch_num:  0\n",
      "MSE:  4.579380160661861  Epoch_num:  1\n",
      "MSE:  4.579380160661861  Epoch_num:  2\n",
      "MSE:  4.579380160661861  Epoch_num:  3\n",
      "MSE:  4.579380160661861  Epoch_num:  4\n",
      "MSE:  4.579380160661861  Epoch_num:  5\n",
      "MSE:  4.579380160661861  Epoch_num:  6\n",
      "MSE:  4.579380160661861  Epoch_num:  7\n",
      "MSE:  4.579380160661861  Epoch_num:  8\n",
      "MSE:  4.579380160661861  Epoch_num:  9\n",
      "MSE:  4.579380160661861  Epoch_num:  10\n",
      "MSE:  4.579380160661861  Epoch_num:  11\n",
      "MSE:  4.579380160661861  Epoch_num:  12\n",
      "MSE:  4.579380160661861  Epoch_num:  13\n",
      "MSE:  4.579380160661861  Epoch_num:  14\n",
      "MSE:  4.579380160661861  Epoch_num:  15\n",
      "MSE:  4.579380160661861  Epoch_num:  16\n",
      "MSE:  4.579380160661861  Epoch_num:  17\n",
      "MSE:  4.579380160661861  Epoch_num:  18\n",
      "MSE:  4.579380160661861  Epoch_num:  19\n",
      "MSE:  4.579380160661861  Epoch_num:  20\n",
      "MSE:  4.579380160661861  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  12.710748434586352  Epoch_num:  0\n",
      "MSE:  6.308898139822024  Epoch_num:  1\n",
      "MSE:  5.347112058413534  Epoch_num:  2\n",
      "MSE:  4.9595477828707395  Epoch_num:  3\n",
      "MSE:  4.772284644907096  Epoch_num:  4\n",
      "MSE:  4.673593072830336  Epoch_num:  5\n",
      "MSE:  4.618808945681419  Epoch_num:  6\n",
      "MSE:  4.5872880092694865  Epoch_num:  7\n",
      "MSE:  4.568645876729106  Epoch_num:  8\n",
      "MSE:  4.557364226720794  Epoch_num:  9\n",
      "MSE:  4.550394764374499  Epoch_num:  10\n",
      "MSE:  4.546003711679858  Epoch_num:  11\n",
      "MSE:  4.543181763014211  Epoch_num:  12\n",
      "MSE:  4.5413299034565755  Epoch_num:  13\n",
      "MSE:  4.540086662868826  Epoch_num:  14\n",
      "MSE:  4.539230676087025  Epoch_num:  15\n",
      "MSE:  4.538624549568876  Epoch_num:  16\n",
      "MSE:  4.538181939633306  Epoch_num:  17\n",
      "MSE:  4.537847940536283  Epoch_num:  18\n",
      "MSE:  4.537587238075563  Epoch_num:  19\n",
      "MSE:  4.53737686333254  Epoch_num:  20\n",
      "MSE:  4.53737686333254  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  20.74721322881894  Epoch_num:  0\n",
      "MSE:  4.567826272338166  Epoch_num:  1\n",
      "MSE:  4.567674363345443  Epoch_num:  2\n",
      "MSE:  4.567674363009752  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  0.1  reg:  10\n",
      "sgd\n",
      "MSE:  19.803922920866288  Epoch_num:  0\n",
      "MSE:  6.2946081249685175  Epoch_num:  1\n",
      "MSE:  6.2946081249685175  Epoch_num:  2\n",
      "MSE:  6.2946081249685175  Epoch_num:  3\n",
      "MSE:  6.2946081249685175  Epoch_num:  4\n",
      "MSE:  6.2946081249685175  Epoch_num:  5\n",
      "MSE:  6.2946081249685175  Epoch_num:  6\n",
      "MSE:  6.2946081249685175  Epoch_num:  7\n",
      "MSE:  6.2946081249685175  Epoch_num:  8\n",
      "MSE:  6.2946081249685175  Epoch_num:  9\n",
      "MSE:  6.2946081249685175  Epoch_num:  10\n",
      "MSE:  6.2946081249685175  Epoch_num:  11\n",
      "MSE:  6.2946081249685175  Epoch_num:  12\n",
      "MSE:  6.2946081249685175  Epoch_num:  13\n",
      "MSE:  6.2946081249685175  Epoch_num:  14\n",
      "MSE:  6.2946081249685175  Epoch_num:  15\n",
      "MSE:  6.2946081249685175  Epoch_num:  16\n",
      "MSE:  6.2946081249685175  Epoch_num:  17\n",
      "MSE:  6.2946081249685175  Epoch_num:  18\n",
      "MSE:  6.2946081249685175  Epoch_num:  19\n",
      "MSE:  6.2946081249685175  Epoch_num:  20\n",
      "MSE:  6.2946081249685175  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  11.325155263534223  Epoch_num:  0\n",
      "MSE:  4.561763809220949  Epoch_num:  1\n",
      "MSE:  4.561442095773651  Epoch_num:  2\n",
      "MSE:  4.564014438535563  Epoch_num:  3\n",
      "MSE:  4.5665522412423405  Epoch_num:  4\n",
      "MSE:  4.568611533875479  Epoch_num:  5\n",
      "MSE:  4.570193215696074  Epoch_num:  6\n",
      "MSE:  4.571378412563594  Epoch_num:  7\n",
      "MSE:  4.572250073773483  Epoch_num:  8\n",
      "MSE:  4.572877111695566  Epoch_num:  9\n",
      "MSE:  4.573313575998309  Epoch_num:  10\n",
      "MSE:  4.5736011396656675  Epoch_num:  11\n",
      "MSE:  4.573771854782235  Epoch_num:  12\n",
      "MSE:  4.573850461199746  Epoch_num:  13\n",
      "MSE:  4.573856169871997  Epoch_num:  14\n",
      "MSE:  4.573804001773126  Epoch_num:  15\n",
      "MSE:  4.573705784701565  Epoch_num:  16\n",
      "MSE:  4.573570895684707  Epoch_num:  17\n",
      "MSE:  4.573406816453251  Epoch_num:  18\n",
      "MSE:  4.573219551796105  Epoch_num:  19\n",
      "MSE:  4.573013947003345  Epoch_num:  20\n",
      "MSE:  4.573013947003345  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  13.699342762410385  Epoch_num:  0\n",
      "MSE:  4.567686830610689  Epoch_num:  1\n",
      "MSE:  4.567674363013331  Epoch_num:  2\n",
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  1  reg:  10\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  14.779381663255329  Epoch_num:  0\n",
      "MSE:  4.623542295446398  Epoch_num:  1\n",
      "MSE:  4.592554957593865  Epoch_num:  2\n",
      "MSE:  4.584139690543319  Epoch_num:  3\n",
      "MSE:  4.581232938679961  Epoch_num:  4\n",
      "MSE:  4.580202876331934  Epoch_num:  5\n",
      "MSE:  4.579893819397559  Epoch_num:  6\n",
      "MSE:  4.579862437910766  Epoch_num:  7\n",
      "MSE:  4.579921266848017  Epoch_num:  8\n",
      "MSE:  4.579987727356243  Epoch_num:  9\n",
      "MSE:  4.580025966257252  Epoch_num:  10\n",
      "MSE:  4.580022018813594  Epoch_num:  11\n",
      "MSE:  4.579972407707049  Epoch_num:  12\n",
      "MSE:  4.579878639079869  Epoch_num:  13\n",
      "MSE:  4.579744456649951  Epoch_num:  14\n",
      "MSE:  4.579574449142233  Epoch_num:  15\n",
      "MSE:  4.579373345726797  Epoch_num:  16\n",
      "MSE:  4.579145669905906  Epoch_num:  17\n",
      "MSE:  4.5788955827281335  Epoch_num:  18\n",
      "MSE:  4.578626826143563  Epoch_num:  19\n",
      "MSE:  4.578342718530138  Epoch_num:  20\n",
      "MSE:  4.578342718530138  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  11.420704150360603  Epoch_num:  0\n",
      "MSE:  4.567688119086262  Epoch_num:  1\n",
      "MSE:  4.567674363040111  Epoch_num:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  10  reg:  10\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  13.976457462154709  Epoch_num:  0\n",
      "MSE:  5.441354905066884  Epoch_num:  1\n",
      "MSE:  5.293299609604113  Epoch_num:  2\n",
      "MSE:  5.198463591128348  Epoch_num:  3\n",
      "MSE:  5.1307156672921925  Epoch_num:  4\n",
      "MSE:  5.078980937063241  Epoch_num:  5\n",
      "MSE:  5.0376718567375836  Epoch_num:  6\n",
      "MSE:  5.003622453914429  Epoch_num:  7\n",
      "MSE:  4.9748837267614325  Epoch_num:  8\n",
      "MSE:  4.9501788558935935  Epoch_num:  9\n",
      "MSE:  4.928629965828697  Epoch_num:  10\n",
      "MSE:  4.909609734969244  Epoch_num:  11\n",
      "MSE:  4.892655552663912  Epoch_num:  12\n",
      "MSE:  4.8774172388992225  Epoch_num:  13\n",
      "MSE:  4.863623815850703  Epoch_num:  14\n",
      "MSE:  4.851061614280688  Epoch_num:  15\n",
      "MSE:  4.839559397497242  Epoch_num:  16\n",
      "MSE:  4.828977981105405  Epoch_num:  17\n",
      "MSE:  4.819202819757367  Epoch_num:  18\n",
      "MSE:  4.810138603572032  Epoch_num:  19\n",
      "MSE:  4.801705247425792  Epoch_num:  20\n",
      "MSE:  4.801705247425792  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  11.423430920176662  Epoch_num:  0\n",
      "MSE:  4.567675957038544  Epoch_num:  1\n",
      "MSE:  4.567674363014565  Epoch_num:  2\n",
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  100  reg:  10\n",
      "sgd\n",
      "adagrad\n",
      "MSE:  14.99483948225864  Epoch_num:  0\n",
      "MSE:  5.61034544411155  Epoch_num:  1\n",
      "MSE:  5.6072017480605805  Epoch_num:  2\n",
      "MSE:  5.6040843691275075  Epoch_num:  3\n",
      "MSE:  5.600992956896335  Epoch_num:  4\n",
      "MSE:  5.597927166981381  Epoch_num:  5\n",
      "MSE:  5.594886660909969  Epoch_num:  6\n",
      "MSE:  5.5918711060072095  Epoch_num:  7\n",
      "MSE:  5.588880175282847  Epoch_num:  8\n",
      "MSE:  5.585913547320204  Epoch_num:  9\n",
      "MSE:  5.58297090616716  Epoch_num:  10\n",
      "MSE:  5.580051941229211  Epoch_num:  11\n",
      "MSE:  5.577156347164568  Epoch_num:  12\n",
      "MSE:  5.574283823781262  Epoch_num:  13\n",
      "MSE:  5.571434075936271  Epoch_num:  14\n",
      "MSE:  5.568606813436633  Epoch_num:  15\n",
      "MSE:  5.565801750942502  Epoch_num:  16\n",
      "MSE:  5.563018607872164  Epoch_num:  17\n",
      "MSE:  5.560257108308939  Epoch_num:  18\n",
      "MSE:  5.557516980909994  Epoch_num:  19\n",
      "MSE:  5.554797958816996  Epoch_num:  20\n",
      "MSE:  5.554797958816996  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  19.779134809545177  Epoch_num:  0\n",
      "MSE:  4.5677593379925625  Epoch_num:  1\n",
      "MSE:  4.567674363199915  Epoch_num:  2\n",
      "MSE:  4.567674363009751  Epoch_num:  3\n",
      "MSE:  4.567674363009751  Epoch_num:  4\n",
      "MSE:  4.567674363009751  Epoch_num:  5\n",
      "MSE:  4.567674363009751  Epoch_num:  6\n",
      "MSE:  4.567674363009751  Epoch_num:  7\n",
      "MSE:  4.567674363009751  Epoch_num:  8\n",
      "MSE:  4.567674363009751  Epoch_num:  9\n",
      "MSE:  4.567674363009751  Epoch_num:  10\n",
      "MSE:  4.567674363009751  Epoch_num:  11\n",
      "MSE:  4.567674363009751  Epoch_num:  12\n",
      "MSE:  4.567674363009751  Epoch_num:  13\n",
      "MSE:  4.567674363009751  Epoch_num:  14\n",
      "MSE:  4.567674363009751  Epoch_num:  15\n",
      "MSE:  4.567674363009751  Epoch_num:  16\n",
      "MSE:  4.567674363009751  Epoch_num:  17\n",
      "MSE:  4.567674363009751  Epoch_num:  18\n",
      "MSE:  4.567674363009751  Epoch_num:  19\n",
      "MSE:  4.567674363009751  Epoch_num:  20\n",
      "MSE:  4.567674363009751  Epoch_num:  21\n",
      "rate:  0.0001  reg:  100\n",
      "sgd\n",
      "MSE:  8.19194529918799  Epoch_num:  0\n",
      "MSE:  6.467825679388069  Epoch_num:  1\n",
      "MSE:  6.467781020995282  Epoch_num:  2\n",
      "MSE:  6.4677810197355265  Epoch_num:  3\n",
      "MSE:  6.467781019735492  Epoch_num:  4\n",
      "MSE:  6.467781019735492  Epoch_num:  5\n",
      "MSE:  6.467781019735492  Epoch_num:  6\n",
      "MSE:  6.467781019735492  Epoch_num:  7\n",
      "MSE:  6.467781019735492  Epoch_num:  8\n",
      "MSE:  6.467781019735492  Epoch_num:  9\n",
      "MSE:  6.467781019735492  Epoch_num:  10\n",
      "MSE:  6.467781019735492  Epoch_num:  11\n",
      "MSE:  6.467781019735492  Epoch_num:  12\n",
      "MSE:  6.467781019735492  Epoch_num:  13\n",
      "MSE:  6.467781019735492  Epoch_num:  14\n",
      "MSE:  6.467781019735492  Epoch_num:  15\n",
      "MSE:  6.467781019735492  Epoch_num:  16\n",
      "MSE:  6.467781019735492  Epoch_num:  17\n",
      "MSE:  6.467781019735492  Epoch_num:  18\n",
      "MSE:  6.467781019735492  Epoch_num:  19\n",
      "MSE:  6.467781019735492  Epoch_num:  20\n",
      "MSE:  6.467781019735492  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  16.232367337884277  Epoch_num:  0\n",
      "MSE:  16.10457031838447  Epoch_num:  1\n",
      "MSE:  16.050682391236094  Epoch_num:  2\n",
      "MSE:  16.00947499695029  Epoch_num:  3\n",
      "MSE:  15.974832865274745  Epoch_num:  4\n",
      "MSE:  15.944386700399056  Epoch_num:  5\n",
      "MSE:  15.916921238819484  Epoch_num:  6\n",
      "MSE:  15.8917145062958  Epoch_num:  7\n",
      "MSE:  15.868296073463785  Epoch_num:  8\n",
      "MSE:  15.846339167050607  Epoch_num:  9\n",
      "MSE:  15.82560585865788  Epoch_num:  10\n",
      "MSE:  15.805916527420214  Epoch_num:  11\n",
      "MSE:  15.787131623100832  Epoch_num:  12\n",
      "MSE:  15.769140166887848  Epoch_num:  13\n",
      "MSE:  15.751852176286906  Epoch_num:  14\n",
      "MSE:  15.735193491868179  Epoch_num:  15\n",
      "MSE:  15.719102135871589  Epoch_num:  16\n",
      "MSE:  15.703525682251692  Epoch_num:  17\n",
      "MSE:  15.688419314618857  Epoch_num:  18\n",
      "MSE:  15.673744364166843  Epoch_num:  19\n",
      "MSE:  15.659467190096864  Epoch_num:  20\n",
      "MSE:  15.659467190096864  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  14.744328634958627  Epoch_num:  0\n",
      "MSE:  6.4671036924091165  Epoch_num:  1\n",
      "MSE:  6.467104072721137  Epoch_num:  2\n",
      "MSE:  6.467104072722273  Epoch_num:  3\n",
      "MSE:  6.467104072722273  Epoch_num:  4\n",
      "MSE:  6.467104072722273  Epoch_num:  5\n",
      "MSE:  6.467104072722273  Epoch_num:  6\n",
      "MSE:  6.467104072722273  Epoch_num:  7\n",
      "MSE:  6.467104072722273  Epoch_num:  8\n",
      "MSE:  6.467104072722273  Epoch_num:  9\n",
      "MSE:  6.467104072722273  Epoch_num:  10\n",
      "MSE:  6.467104072722273  Epoch_num:  11\n",
      "MSE:  6.467104072722273  Epoch_num:  12\n",
      "MSE:  6.467104072722273  Epoch_num:  13\n",
      "MSE:  6.467104072722273  Epoch_num:  14\n",
      "MSE:  6.467104072722273  Epoch_num:  15\n",
      "MSE:  6.467104072722273  Epoch_num:  16\n",
      "MSE:  6.467104072722273  Epoch_num:  17\n",
      "MSE:  6.467104072722273  Epoch_num:  18\n",
      "MSE:  6.467104072722273  Epoch_num:  19\n",
      "MSE:  6.467104072722273  Epoch_num:  20\n",
      "MSE:  6.467104072722273  Epoch_num:  21\n",
      "rate:  0.001  reg:  100\n",
      "sgd\n",
      "MSE:  13.270596706360374  Epoch_num:  0\n",
      "MSE:  6.46550759963912  Epoch_num:  1\n",
      "MSE:  6.46550759963912  Epoch_num:  2\n",
      "MSE:  6.46550759963912  Epoch_num:  3\n",
      "MSE:  6.46550759963912  Epoch_num:  4\n",
      "MSE:  6.46550759963912  Epoch_num:  5\n",
      "MSE:  6.46550759963912  Epoch_num:  6\n",
      "MSE:  6.46550759963912  Epoch_num:  7\n",
      "MSE:  6.46550759963912  Epoch_num:  8\n",
      "MSE:  6.46550759963912  Epoch_num:  9\n",
      "MSE:  6.46550759963912  Epoch_num:  10\n",
      "MSE:  6.46550759963912  Epoch_num:  11\n",
      "MSE:  6.46550759963912  Epoch_num:  12\n",
      "MSE:  6.46550759963912  Epoch_num:  13\n",
      "MSE:  6.46550759963912  Epoch_num:  14\n",
      "MSE:  6.46550759963912  Epoch_num:  15\n",
      "MSE:  6.46550759963912  Epoch_num:  16\n",
      "MSE:  6.46550759963912  Epoch_num:  17\n",
      "MSE:  6.46550759963912  Epoch_num:  18\n",
      "MSE:  6.46550759963912  Epoch_num:  19\n",
      "MSE:  6.46550759963912  Epoch_num:  20\n",
      "MSE:  6.46550759963912  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  17.021318521700533  Epoch_num:  0\n",
      "MSE:  15.791410525452577  Epoch_num:  1\n",
      "MSE:  15.303819186157467  Epoch_num:  2\n",
      "MSE:  14.943569772126628  Epoch_num:  3\n",
      "MSE:  14.649243156747913  Epoch_num:  4\n",
      "MSE:  14.39702152949775  Epoch_num:  5\n",
      "MSE:  14.174672671782217  Epoch_num:  6\n",
      "MSE:  13.974918078357991  Epoch_num:  7\n",
      "MSE:  13.793006629191787  Epoch_num:  8\n",
      "MSE:  13.625631782327645  Epoch_num:  9\n",
      "MSE:  13.470381485516086  Epoch_num:  10\n",
      "MSE:  13.325431685180517  Epoch_num:  11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  13.189363248196894  Epoch_num:  12\n",
      "MSE:  13.061046464103573  Epoch_num:  13\n",
      "MSE:  12.939564893210877  Epoch_num:  14\n",
      "MSE:  12.8241632899621  Epoch_num:  15\n",
      "MSE:  12.714210878325394  Epoch_num:  16\n",
      "MSE:  12.609174764465326  Epoch_num:  17\n",
      "MSE:  12.508600247066523  Epoch_num:  18\n",
      "MSE:  12.412095945068879  Epoch_num:  19\n",
      "MSE:  12.319322368019057  Epoch_num:  20\n",
      "MSE:  12.319322368019057  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  13.227679111935748  Epoch_num:  0\n",
      "MSE:  6.467103795057783  Epoch_num:  1\n",
      "MSE:  6.46710407272141  Epoch_num:  2\n",
      "MSE:  6.467104072722273  Epoch_num:  3\n",
      "MSE:  6.467104072722273  Epoch_num:  4\n",
      "MSE:  6.467104072722273  Epoch_num:  5\n",
      "MSE:  6.467104072722273  Epoch_num:  6\n",
      "MSE:  6.467104072722273  Epoch_num:  7\n",
      "MSE:  6.467104072722273  Epoch_num:  8\n",
      "MSE:  6.467104072722273  Epoch_num:  9\n",
      "MSE:  6.467104072722273  Epoch_num:  10\n",
      "MSE:  6.467104072722273  Epoch_num:  11\n",
      "MSE:  6.467104072722273  Epoch_num:  12\n",
      "MSE:  6.467104072722273  Epoch_num:  13\n",
      "MSE:  6.467104072722273  Epoch_num:  14\n",
      "MSE:  6.467104072722273  Epoch_num:  15\n",
      "MSE:  6.467104072722273  Epoch_num:  16\n",
      "MSE:  6.467104072722273  Epoch_num:  17\n",
      "MSE:  6.467104072722273  Epoch_num:  18\n",
      "MSE:  6.467104072722273  Epoch_num:  19\n",
      "MSE:  6.467104072722273  Epoch_num:  20\n",
      "MSE:  6.467104072722273  Epoch_num:  21\n",
      "rate:  0.01  reg:  100\n",
      "sgd\n",
      "MSE:  13.684495564464422  Epoch_num:  0\n",
      "MSE:  6.658447167971023  Epoch_num:  1\n",
      "MSE:  6.658447167971023  Epoch_num:  2\n",
      "MSE:  6.658447167971023  Epoch_num:  3\n",
      "MSE:  6.658447167971023  Epoch_num:  4\n",
      "MSE:  6.658447167971023  Epoch_num:  5\n",
      "MSE:  6.658447167971023  Epoch_num:  6\n",
      "MSE:  6.658447167971023  Epoch_num:  7\n",
      "MSE:  6.658447167971023  Epoch_num:  8\n",
      "MSE:  6.658447167971023  Epoch_num:  9\n",
      "MSE:  6.658447167971023  Epoch_num:  10\n",
      "MSE:  6.658447167971023  Epoch_num:  11\n",
      "MSE:  6.658447167971023  Epoch_num:  12\n",
      "MSE:  6.658447167971023  Epoch_num:  13\n",
      "MSE:  6.658447167971023  Epoch_num:  14\n",
      "MSE:  6.658447167971023  Epoch_num:  15\n",
      "MSE:  6.658447167971023  Epoch_num:  16\n",
      "MSE:  6.658447167971023  Epoch_num:  17\n",
      "MSE:  6.658447167971023  Epoch_num:  18\n",
      "MSE:  6.658447167971023  Epoch_num:  19\n",
      "MSE:  6.658447167971023  Epoch_num:  20\n",
      "MSE:  6.658447167971023  Epoch_num:  21\n",
      "adagrad\n",
      "MSE:  9.92456541623158  Epoch_num:  0\n",
      "MSE:  6.715209065722538  Epoch_num:  1\n",
      "MSE:  6.53400346022821  Epoch_num:  2\n",
      "MSE:  6.491604468633521  Epoch_num:  3\n",
      "MSE:  6.478575142464387  Epoch_num:  4\n",
      "MSE:  6.47399491674568  Epoch_num:  5\n",
      "MSE:  6.472254573650636  Epoch_num:  6\n",
      "MSE:  6.471533114714508  Epoch_num:  7\n",
      "MSE:  6.471185771063807  Epoch_num:  8\n",
      "MSE:  6.4709767688568665  Epoch_num:  9\n",
      "MSE:  6.470819425464864  Epoch_num:  10\n",
      "MSE:  6.470682238083881  Epoch_num:  11\n",
      "MSE:  6.4705540878798535  Epoch_num:  12\n",
      "MSE:  6.4704312676608176  Epoch_num:  13\n",
      "MSE:  6.470312670349207  Epoch_num:  14\n",
      "MSE:  6.470198033325145  Epoch_num:  15\n",
      "MSE:  6.470087319596478  Epoch_num:  16\n",
      "MSE:  6.46998051524991  Epoch_num:  17\n",
      "MSE:  6.469877574555486  Epoch_num:  18\n",
      "MSE:  6.469778414066883  Epoch_num:  19\n",
      "MSE:  6.469682920524823  Epoch_num:  20\n",
      "MSE:  6.469682920524823  Epoch_num:  21\n",
      "adadelta\n",
      "MSE:  14.130223669515122  Epoch_num:  0\n",
      "MSE:  6.467103808440132  Epoch_num:  1\n",
      "MSE:  6.467104072721461  Epoch_num:  2\n",
      "MSE:  6.467104072722273  Epoch_num:  3\n",
      "MSE:  6.467104072722273  Epoch_num:  4\n",
      "MSE:  6.467104072722273  Epoch_num:  5\n",
      "MSE:  6.467104072722273  Epoch_num:  6\n",
      "MSE:  6.467104072722273  Epoch_num:  7\n",
      "MSE:  6.467104072722273  Epoch_num:  8\n",
      "MSE:  6.467104072722273  Epoch_num:  9\n",
      "MSE:  6.467104072722273  Epoch_num:  10\n",
      "MSE:  6.467104072722273  Epoch_num:  11\n",
      "MSE:  6.467104072722273  Epoch_num:  12\n",
      "MSE:  6.467104072722273  Epoch_num:  13\n",
      "MSE:  6.467104072722273  Epoch_num:  14\n",
      "MSE:  6.467104072722273  Epoch_num:  15\n",
      "MSE:  6.467104072722273  Epoch_num:  16\n",
      "MSE:  6.467104072722273  Epoch_num:  17\n",
      "MSE:  6.467104072722273  Epoch_num:  18\n",
      "MSE:  6.467104072722273  Epoch_num:  19\n",
      "MSE:  6.467104072722273  Epoch_num:  20\n",
      "MSE:  6.467104072722273  Epoch_num:  21\n",
      "rate:  0.1  reg:  100\n",
      "sgd\n",
      "MSE:  12.780959186665827  Epoch_num:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:75: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:76: RuntimeWarning: overflow encountered in add\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:79: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:77: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:78: RuntimeWarning: overflow encountered in double_scalars\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:86: RuntimeWarning: invalid value encountered in subtract\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:87: RuntimeWarning: invalid value encountered in subtract\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:90: RuntimeWarning: invalid value encountered in subtract\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-1710e3dedc44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sgd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreg\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"SGD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m             \u001b[0mnew_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mse\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmse_sgd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-1710e3dedc44>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, update_rule)\u001b[0m\n\u001b[1;32m     48\u001b[0m                 \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_num\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-120-1710e3dedc44>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#may want to do deep copy here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \"\"\"\n\u001b[1;32m    237\u001b[0m     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n\u001b[0;32m--> 238\u001b[0;31m         y_true, y_pred, multioutput)\n\u001b[0m\u001b[1;32m    239\u001b[0m     output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n\u001b[1;32m    240\u001b[0m                                weights=sample_weight)\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/metrics/regression.py\u001b[0m in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput)\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    451\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate, regularization_constant, epochs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.epochs = epochs\n",
    "        self.plot_weights = []\n",
    "        self.plot_errors = []\n",
    "        self.plot_epoch = []\n",
    "        self.epoch_num = 0\n",
    "        self.weights = np.random.random_sample(5,)\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.update_mode = 0\n",
    "        self.cache = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.cache = np.array(self.cache)\n",
    "        self.gradient_ms = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.gradient_ms = np.array(self.gradient_ms)\n",
    "        self.parameter_ms = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.parameter_ms = np.array(self.parameter_ms)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, update_rule=\"SGD\"):\n",
    "        if(update_rule == 'adagrad'):\n",
    "            self.update_mode = 1\n",
    "        elif(update_rule == 'adadelta'):\n",
    "            self.update_mode = 2\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.evaluate_model()\n",
    "        self.epoch_num = self.epoch_num + 1\n",
    "        while (self.epoch_num <= self.epochs):\n",
    "            for index in range(len(X)):\n",
    "                truth = y[index]\n",
    "                features = X[index]\n",
    "                #print(\"This is truth: \", truth)\n",
    "                #print(\"This is features: \", features)\n",
    "                prediction = self.predict_one(features)\n",
    "                #print(\"This is prediction: \", prediction)\n",
    "                error = self.compute_error(truth,prediction)\n",
    "                gradients = self.compute_gradients(error, features)\n",
    "                self.update_weights(gradients)\n",
    "            self.evaluate_model()\n",
    "            self.epoch_num = self.epoch_num + 1\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        predictions = self.predict_all(X)\n",
    "        mse = mean_squared_error(y,predictions)\n",
    "        self.plot_errors.append(mse)\n",
    "        self.plot_weights.append(self.weights) #may want to do deep copy here\n",
    "        self.plot_epoch.append(self.epoch_num)\n",
    "        print(\"MSE: \", mse, \" Epoch_num: \", self.epoch_num)\n",
    "        return mse\n",
    "    def predict_all(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self.predict_one(x))\n",
    "        return predictions\n",
    "            \n",
    "    def predict_one(self, X):\n",
    "        return self.weights[0] + X[0]*self.weights[1] + X[0]*X[0]*self.weights[2] \\\n",
    "            + X[1]*X[1]*self.weights[3] + X[1]*self.weights[4] \n",
    "        \n",
    "    def compute_error(self, truth, prediction):\n",
    "        return prediction - truth\n",
    "    \n",
    "    def compute_gradients(self, error, features):\n",
    "        gradients = []\n",
    "        gradients.append(2*error + self.regularization_constant*self.weights[0])\n",
    "        gradients.append(2*features[0]*error + self.regularization_constant*self.weights[1])\n",
    "        gradients.append(2*features[0]*features[0]*error + self.regularization_constant*self.weights[2])\n",
    "        gradients.append(2*features[1]*features[1]*error + self.regularization_constant*self.weights[3])\n",
    "        gradients.append(2*features[1]*error + self.regularization_constant*self.weights[4])\n",
    "        #print(\"These are gradients: \", gradients)\n",
    "        return gradients\n",
    "    \n",
    "    def update_weights(self, gradients):\n",
    "        #print(\"Beginning weights: \", self.weights)\n",
    "        if(self.update_mode == 0):\n",
    "            self.weights[0] = self.weights[0] - self.learning_rate*gradients[0] \n",
    "            self.weights[1] = self.weights[1] - self.learning_rate*gradients[1]\n",
    "            self.weights[2] = self.weights[2] - self.learning_rate*gradients[2]\n",
    "            self.weights[3] = self.weights[3] - self.learning_rate*gradients[3]\n",
    "            self.weights[4] = self.weights[4] - self.learning_rate*gradients[4]\n",
    "        if(self.update_mode == 1):\n",
    "            gradients_squared = np.multiply(gradients,gradients)\n",
    "            #print(\"This is gradients_squared \", gradients_squared)\n",
    "            #print(\"This  was self.cache \", self.cache)\n",
    "            self.cache = np.add(self.cache, gradients_squared)\n",
    "            #print(\"This is now self.cache \", self.cache)\n",
    "            self.weights[0] = self.weights[0] - self.learning_rate*gradients[0]/math.sqrt(self.cache[0] + 1e-6)\n",
    "            self.weights[1] = self.weights[1] - self.learning_rate*gradients[1]/math.sqrt(self.cache[1] + 1e-6)\n",
    "            self.weights[2] = self.weights[2] - self.learning_rate*gradients[2]/math.sqrt(self.cache[2] + 1e-6)\n",
    "            self.weights[3] = self.weights[3] - self.learning_rate*gradients[3]/math.sqrt(self.cache[3] + 1e-6)\n",
    "            self.weights[4] = self.weights[4] - self.learning_rate*gradients[4]/math.sqrt(self.cache[4] + 1e-6)\n",
    "        if(self.update_mode == 2):\n",
    "            update_vals = [[0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "            update_vals = np.array(update_vals)\n",
    "            self.gradient_ms = np.add(np.multiply(0.9, self.gradient_ms), np.multiply(0.1,np.multiply(gradients,gradients)))\n",
    "            update_vals[0] = -1*gradients[0]*math.sqrt(self.parameter_ms[0] + 1e-6)/math.sqrt(self.gradient_ms[0] + 1e-6)\n",
    "            update_vals[1] = -1*gradients[1]*math.sqrt(self.parameter_ms[1] + 1e-6)/math.sqrt(self.gradient_ms[1] + 1e-6)\n",
    "            update_vals[2] = -1*gradients[2]*math.sqrt(self.parameter_ms[2] + 1e-6)/math.sqrt(self.gradient_ms[2] + 1e-6)\n",
    "            update_vals[3] = -1*gradients[3]*math.sqrt(self.parameter_ms[3] + 1e-6)/math.sqrt(self.gradient_ms[3] + 1e-6)\n",
    "            update_vals[4] = -1*gradients[4]*math.sqrt(self.parameter_ms[4] + 1e-6)/math.sqrt(self.gradient_ms[4] + 1e-6)\n",
    "            #update parameter rms\n",
    "            self.parameter_ms = np.add(np.multiply(0.9, self.parameter_ms), np.multiply(0.1,np.multiply(update_vals,update_vals)))\n",
    "            #print(\"gradients: \", gradients)\n",
    "            #print(\"update_vals: \", update_vals)\n",
    "            #print(\"gradient_ms: \", self.gradient_ms)\n",
    "            #print(\"parameter_ms: \", self.parameter_ms)\n",
    "            self.weights[0] = self.weights[0] + update_vals[0]\n",
    "            self.weights[1] = self.weights[1] + update_vals[1]\n",
    "            self.weights[2] = self.weights[2] + update_vals[2]\n",
    "            self.weights[3] = self.weights[3] + update_vals[3]\n",
    "            self.weights[4] = self.weights[4] + update_vals[4]\n",
    "        #print(\"Ending weights: \", self.weights)\n",
    "    \n",
    "    \n",
    "print (\"Hello\")\n",
    "df = pd.read_csv('samples.csv')\n",
    "df = df.dropna()\n",
    "X = df[[u'x_1',u'x_2']]\n",
    "X = X.as_matrix()\n",
    "X = X.astype('float')\n",
    "y = df[[u'y']]\n",
    "y = y.as_matrix()\n",
    "y = np.array(y)\n",
    "y = y.astype('float')\n",
    "regularizations = [0,10,100]\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "mse_sgd = 10000000\n",
    "sgd_rate = 0\n",
    "sgd_reg = 0\n",
    "mse_adagrad = 10000000\n",
    "adagrad_rate = 0\n",
    "adagrad_reg = 0\n",
    "mse_adadelta = 10000000\n",
    "adadelta_rate = 0\n",
    "adadelta_reg = 0\n",
    "for reg in regularizations:\n",
    "    for rate in learning_rates:\n",
    "        print(\"rate: \", rate, \" reg: \", reg)\n",
    "        sgd = SGD(rate,reg,20)\n",
    "        print(\"sgd\")\n",
    "        if((rate < 1 and reg != 0) or reg == 100):\n",
    "            sgd.fit(X,y,\"SGD\")\n",
    "            new_mse = sgd.evaluate_model()\n",
    "            if(new_mse < mse_sgd):\n",
    "                mse_sgd = new_mse\n",
    "                sgd_rate = rate\n",
    "                sgd_reg = reg\n",
    "        sgd = SGD(rate,reg,20)\n",
    "        print(\"adagrad\")\n",
    "        sgd.fit(X,y,\"adagrad\")\n",
    "        new_mse = sgd.evaluate_model()\n",
    "        if(new_mse < mse_adagrad):\n",
    "            mse_adagrad = new_mse\n",
    "            adagrad_rate = rate\n",
    "            adagrad_reg = reg\n",
    "        sgd = SGD(rate,reg,20)\n",
    "        print(\"adadelta\")\n",
    "        sgd.fit(X,y,\"adadelta\")\n",
    "        new_mse = sgd.evaluate_model()\n",
    "        if(new_mse < mse_adadelta):\n",
    "            mse_adadelta = new_mse\n",
    "            adadelta_rate = rate\n",
    "            adadelta_reg = reg\n",
    "            \n",
    "print(\"sgd optimal mse: \", mse_sgd, \" rate: \", sgd_rate, \"reg: \", sgd_reg)\n",
    "print(\"adagrad optimal mse: \", mse_adagrad, \" rate: \", adagrad_rate, \"reg: \", adagrad_reg)\n",
    "print(\"adadelta optimal mse: \", mse_adadelta, \" rate: \", adadelta_rate, \"reg: \", adadelta_reg)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. (6+4=10 pts) Tensor Playground\n",
    "Visit http://playground.tensorflow.org for this problem\n",
    "\n",
    "A. From the far right, select \"Regression\" as the problem type, and select the 2nd of the two data sets ( the right one ).  \n",
    "\n",
    "   i) What sort of test / training loss do you get if you run it for 200 epochs with the following learning rates: .3, .01 and .003 ?  What if you run it for 1000 epochs with these learning rates?  Leave all other values at their defaults ( test/training ratio 50%, Noise 0, Batch Size 10, using Tanh activation function, and No Regularization )\n",
    "   \n",
    "   ii) Keeping learning rate at .3, Activation at Tanh, with all others at their defaults, and running for 200 epochs.  \n",
    "     What sort of test/train loss can you achieve using only 1 neuron in the first hidden layer.  What about for 2,3 or 8 neurons?  Provide screen shots of output layer and comment on how the different output fits look and compare with one another.\n",
    "\n",
    "   iii)Now keeping learning rate at .03 with all others at their defaults, and running for 200 epochs.  \n",
    "       Compare how the activation functions affect the ouput ( ReLU, Sigmoid, Tanh, Linear ). Provide screen shots of output results and comment.\n",
    "\n",
    "\n",
    "B. Neural Nets can fit anything.  Now reset to the initial defaults, and select \"Classification\" as the problem type, and from the Data section, select the bottom right \"Spriral\" data set.  With the idea of trying to minimize training/testing error, provide solutions to the problem for the following 2 scenarios.  i) Using just the first 2 inputs ( as per default ) and ii) Using all 7 of the inputs.  You may use as many layers as you want, whatever activation, however man neurons.  Provide screen shots which show your full network, output and parameters. Briefly justify your decisions, and comment on difficulties/tradeoffs, what helps/what doesn't,etc. \n",
    "\n",
    "## ANSWER\n",
    "\n",
    "### i\n",
    "200 Epochs\n",
    "0.3 - test loss: 0.020, training loss: 0.016\n",
    "\n",
    "0.01 - test loss: .04, training loss: 0.03\n",
    "\n",
    "0.003 - test loss: 0.046, training loss: 0.036\n",
    "\n",
    "1000 Epochs\n",
    "0.3 - test loss: 0.012, training loss: 0.009\n",
    "\n",
    "0.01 - test loss: 0.019, training loss: 0.016\n",
    "\n",
    "0.003 - test loss: 0.024, training loss: 0.019\n",
    "\n",
    "### ii\n",
    "1 Neuron in 1st Hidden Layer\n",
    "\n",
    "test: 0.041, training: 0.038\n",
    "\n",
    "We can see that with just one neuron in the hidden layer the model lacks power in order to accurately predict this dataset. This is because the limited number of weights can not represent this function. \n",
    "<img src=\"2_1.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Neurons in Hidden Layer\n",
    "\n",
    "test: 0.029, training: 0.027\n",
    "\n",
    "As we add more features the models complexity increases and so does the power to represent more complicated functions. As we increase the number of weights our power increases and we can represent more complicated functions.\n",
    "<img src=\"2_2.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 neurons in Hidden Layer\n",
    "\n",
    "test: 0.022, training: 0.020\n",
    "\n",
    "This was a very big improvement, now we start to see the model correctly identifying the 3 very blue and 3 very orange areas. \n",
    "\n",
    "<img src=\"2_3.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 neurons in Hidden Layer\n",
    "\n",
    "test: 0.016, training: 0.015\n",
    "\n",
    "The model was already doing pretty well, which is why we do not see as large an improvement from 3 neurons to 8 neurons as from 2 to 3. We can see from this lowered benefit that we are near or already passed the threshold of the most powerful network which we need. It is very possible with increased complexity we may see no benefit at all because of overfitting.\n",
    "<img src=\"2_4.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii\n",
    "\n",
    "tanh, test loss: 0.022, training loss 0.020\n",
    "\n",
    "tanh does the best of any function for this problem. We see it does the best at generalizing the data as well in that the boundaries look more natural. \n",
    "<img src=\"2_5.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RelU, test loss: 0.027, training loss 0.027\n",
    "\n",
    "RelU is very powerful because of its ability to not saturate. However, we also have the problem of neurons dying because of them falling into the less than 0 region of the RelU. \n",
    "\n",
    "<img src=\"2_6.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid, test loss: 0.041, training loss 0.044\n",
    "\n",
    "Sigmoid really struggles with this data set. I believed simgoid was supposed to be quite good, however it looks like it really struggles. This is possibly because of it not being zero centered which restricts later nodes in the model because no negative values can come out of the non linearity. \n",
    "\n",
    "<img src=\"2_7.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear, test loss: 0.040, training loss 0.044\n",
    "\n",
    "It is clear to see why linear would struggle with this problem given the data we are trying to model is clearly non linear.\n",
    "\n",
    "<img src=\"2_8.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b part i\n",
    "For this problem I knew I was going to need a deep and complex network in order to get a good solution. A lot of weights would be needed in order to turn just 2 features into this output. I also felt that depth would be good in order to generalize the shape of the spiral. Because I figured I would need a large and deep network, I though RelU would be better to help with the weight decay problem when using the tanh function. However, it did take me quite a few tries when running this network to get a really good answer because the space in which to get lost in is so much larger with this complicated of a network. \n",
    "<img src=\"2_9.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii\n",
    "For this problem I chose to go with a smaller and less deep network. Because of the inclusion of the other features, there are now non linear functions which can really help when trying to predict something complicated like a spiral. The two sin functions are very beneficial because of their circular nature too. Because of these additional features, I felt less neurons would be needed. Because less neurons were needed and the depth of the network was not too large, I believed a tanh function would do quite well. I did have to run this one a few times to get a really good answer, but not nearly as many times as the network in part i.\n",
    "<img src=\"2_10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi-layer Perceptron Regressor (15 points)\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the OpenCL gemm kernel performance prediction dataset for this problem https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance.\n",
    "\n",
    "Following code will pre-process the data and split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 42** and **test_size = 0.33**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((161872, 14), (161872,), (79728, 14), (79728,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('sgemm_product.csv') \n",
    "data['target'] = 0.25*(data['Run1 (ms)'] + data['Run2 (ms)']+ data['Run3 (ms)'] + data['Run4 (ms)'])\n",
    "y = data['target']\n",
    "X = data.drop(['target','Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y.\n",
    "\n",
    "Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=5000, learning_rate_init = 0.005\n",
    "    \n",
    "Now, consider the following settings for the number of hidden units:\n",
    "    \n",
    "        (2,), (10,), (20,), (30,), (40,)\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model (5pts)\n",
    "      \n",
    "2) Now, using the same number of hidden units used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works the best? Briefly analyze the result in terms of the number of hidden units. (3pts)\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves for model (a), (b), and (c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_* value. (2pts)\n",
    "\n",
    "4) Activation Layer: Use the tanh and relu activations for the following fixed parameters to train your model:\n",
    "    \n",
    "    solver = 'sgd', random_state=42,\n",
    "    batch_size=5000, hidden_layer_sizes = (2,)\n",
    "\n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model. (2pts)\n",
    "   \n",
    "5) Optimization technique: The sklearn API performs a momentum update when using SGD. To understand the difference in performance of vanilla SGD and SGD with momentum, set momentum to 0 with SGD. Here, we compare the performance of vanilla sgd against adam solvers. Since both of these use different optimization subroutines, the learning rate must also be varied. In this question, vary the learning rate through these values [1, 0.1, 0.01, 0.001]. Keep the following fixed parameters to train your model:\n",
    "\n",
    "    activation = 'tanh', random_state=42,\n",
    "    batch_size=5000, hidden_layer_sizes = (2,), momentum = 0\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for the best learning rate for each model. (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Multi-level Model in Python (20 points)\n",
    "In this problem, you will explore multi-level model using a python package [PyMC3](https://pymc-devs.github.io/pymc3/index.html#). The dataset \"brew_by_year.csv\" will be used in this question.  This dataset contains fields:\n",
    "\n",
    "- `state`\n",
    "- `state_id`\n",
    "- `brew`\n",
    "- `year`\n",
    "\n",
    "Where 'brew' is the number of breweries in the state that year, and `state_id` is a unique ID for that state.\n",
    "\n",
    "(a)  (2 pts) Plot the relationship between number of breweries and year, and draw a linearly regressed line ignoring the ID variable.\n",
    "\n",
    "(b)  (2 pts) Plot the relationship between number of breweries and year, but this time, fit a different linear regression for each state.\n",
    "\n",
    "(c)  (2 + 4 + 6 pts) Divide the dataset into training and test sets.  The training set contains the first 6 years of the measurements, and the test set contains the rest of the measurements. \n",
    "Build three different linear models:\n",
    "- Global model:  a linear model using `brew` as the dependent variable and `year` as the independent variable. Pool all data and estimate one common regression model to assess the influence of the passage of time across the total number of breweries.\n",
    "- Local model:  a different linear model for each state i.e., 51 different linear regressions. We are interested in whether different states actually follow separate regression models.\n",
    "- Multilevel model:  Use the [PyMC3](http://pymc-devs.github.io/pymc3/notebooks/GLM-hierarchical.html#Partial-pooling:-Hierarchical-Regression-aka,-the-best-of-both-worlds) package to fit a multilevel model specified as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{brew}_{it} &= \\beta_{0i} + \\beta_{1i} \\text{year}_{it} + \\epsilon_{it}\\\\\n",
    "\\beta_{0i} &= \\beta_{00}  + \\eta_{0i} \\\\\n",
    "\\beta_{1i} &= \\beta_{10} + \\eta_{1i} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Predict the number of breweries for the next 2 years, and calculate the mean squared errors from the three models.\n",
    "\n",
    "(d)  (2 pts) Briefly state what do $\\beta_{00}$ and $\\beta_{10}$ mean in this multilevel model.\n",
    "\n",
    "(e) (2 pt) Visually assess from the trace plot whether the MLM specified in the problem is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('brew_by_year.csv', index_col=0)\n",
    "data['brew'] = (data['brew'] - np.mean(data['brew'])) / np.std(data['brew'])\n",
    "\n",
    "# Specify the hyper-parameter of Multilevel model:\n",
    "beta_00 = pm.Normal('mu_beta0', mu=0., sd=10)\n",
    "eta_0i = pm.HalfCauchy('eta_0', 5)\n",
    "beta_10 = pm.Normal('mu_beta1', mu=0., sd=10)\n",
    "eta_1i = pm.HalfCauchy('eta1', 5)\n",
    "    \n",
    "# Intercept and Slope\n",
    "beta_0i = pm.Normal('beta0', mu=beta_00, sd=eta_0i, shape=len(train_data.state_id.unique()))\n",
    "beta_1i = pm.Normal('beta1', mu=beta_10, sd=eta_1i, shape=len(train_data.state_id.unique()))    \n",
    "# Model error\n",
    "eps = pm.HalfCauchy('eps', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Decision Tree using Python (5 pts)\n",
    "In this problem, you will model the data using decision trees to perform a classification task on the energy dataset provided. The dataset has been preprocessed. Using the class tree.DecisionTreeClassifier (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree), build two different trees with a maximum depth of two using the split criteria (i) Gini and (ii) Entropy. Use entire data to build trees. Plot the two trees. If your classifier object is called clf, use the following commands to save the generated tree as a '.dot' file that can be used to visualize the tree using Webgraphviz: http://www.webgraphviz.com/\n",
    "\n",
    "Hint: see  http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "with open(\"decision_tree_gini.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste the contents of your '.dot' file into the text window on the website to visualize the trees. At which node(s) do they differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('energydata_complete.csv') \n",
    "\n",
    "y = data['Appliances']\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis = 1)\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "timeData = np.array(data['date'])\n",
    "\n",
    "days = []\n",
    "hours = []\n",
    "for line in xrange(len(timeData)):\n",
    "    day = parser.parse(timeData[line]).weekday()\n",
    "    hour = parser.parse(timeData[line]).hour\n",
    "    days.append(day)\n",
    "    hours.append(hour)\n",
    "    \n",
    "X = pd.concat([X, pd.get_dummies(days), pd.get_dummies(hours)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6. (2+5+3=10 pts) Bayes Decision Theory\n",
    "a. Explain what you understand by class-conditional likelihood, class priors, and posterior probability of a class given an input, and the relationship between them. Please define all symbols and equations used explicitly.\n",
    "( 2 points )\n",
    "\n",
    "b. Suppose you want to learn a binary classifier to predict whether or not a customer will buy a TV. The class label is 0 if the customer does not buy a TV and 1 if they do. For each customer, you are given two features, $x_1$ is the per hour salary and $x_2$ is the age. Assume that the class conditional distribution $p([x_1 , x_2]|C)$ is Gaussian. The mean salary and age of the people who do buy a TV is 15 and 40 respectively and that of those who dont is 8.5 and 25. The first class of customers, $\\sigma_1$ = 1, $\\sigma_2 = 2$ and $\\rho = 0$. For the second class of customers, $\\sigma_1$ = 3, $\\sigma_2 = 2$ and $\\rho = 0$.(Refer HW1 on how to construct a covariance matrix with this information). Further, your sales data suggests that only 1 in 3 people actually bought a TV in the last few years. Mathematically derive the (optimal) Bayes decision boundary for this problem. (5 points)\n",
    "\n",
    "c. Now sample 1000 customers from each class (C = 0, 1) under the assumed distribution and the estimated parameters and plot their features. Additionally, plot the decision boundary you obtained in the part (b) on the same plot. (3 points)\n",
    "\n",
    "## ANSWER\n",
    "\n",
    "### a\n",
    "Class conditional likelihood function - $p([x_1,x_2]|C_i)$ \n",
    "This is likelihood of $x_1, x_2$ given the class. \n",
    "\n",
    "Class priors - $P(C_i)$\n",
    "This is the likelihood a given datapoint is apart of class $Ci$\n",
    "\n",
    "Posterior probability - $p(C_i|[x_1,x_2]) = \\frac{P(C_i)p([x_1,x_2]|C_i)}{p([x_1, x_2])}$\n",
    "This is the likelihood that for a given $x_1, x_2$ the datapoint is apart of class $C_i$. The optimal decision for each $x_1, x_2$ is to choose the $p(C_i|[x_1,x_2])$ which is greatest for that $x_1 and x_2$. \n",
    "\n",
    "Bayes decision boundary (boundaries) = $P(C_0| [x_1,x_2]) = P(C_1| [x_1,x_2])$\n",
    "We need to choose whichever posterior probability is higher, so whenever these quantities are equal, this will be our decision boundary. When solving for this boundary we do not even need to compare the denominators in the posterior probability because they will cancel out. Therefor this equation is really just $P(C_0)p([x_1,x_2]|C_0) = P(C_1)p([x_1,x_2]|C_1)$\n",
    "\n",
    "### b\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation} \\mu_0 = \\left[ \\begin{array}{cc} 15 \\\\ 40  \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_0 = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 4 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_0^{-1} = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 0.25 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "$\\mid\\Sigma_0\\mid = 4$\n",
    "\n",
    "\\begin{equation} \\mu_1 = \\left[ \\begin{array}{cc} 8.5 \\\\ 25  \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_1 = \\left[ \\begin{array}{cc} 9 & 0 \\\\ 0 & 4 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_1^{-1} = \\left[ \\begin{array}{cc} 0.111 & 0 \\\\ 0 & 0.25 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "$\\mid\\Sigma_1\\mid = 36$\n",
    "\n",
    "$P(C_0) = 2/3$\n",
    "\n",
    "$P(C_1) = 1/3$\n",
    "\n",
    "$P(C_0)p([x_1,x_2]|C_0) = P(C_1)p([x_1,x_2]|C_1)$\n",
    "\n",
    "$2/3N(\\mu_0,\\Sigma_0) = 1/3N(\\mu_1,\\Sigma_1)$\n",
    "\n",
    "$\\ln(2/3) + \\ln(N(\\mu_0,\\Sigma_0)) = \\ln(1/3) + \\ln(N(\\mu_1,\\Sigma_1))$\n",
    "\n",
    "$\\ln(2/3)+-1/2(\\vec{x}-\\mu_0)^T\\Sigma^{-1}_0(\\vec{x}-\\mu_0)\\ln(2\\pi\\mid\\Sigma_0\\mid^{1/2})=\\ln(1/3) + -1/2(\\vec{x} - \\mu_1)^T\\Sigma^{-1}_1(\\vec{x}-\\mu_1)\\ln(2\\pi\\mid\\Sigma_1\\mid^{1/2})$\n",
    "\n",
    "A lot of reduction that I did by hand, included at end\n",
    "\n",
    "$-1.265x_1^2 + 37.95x_1 - 0.316x_2^2 + 25.3x_2 - 791.03 = -0.2x_1^2 + 3.43x_1 - 0.454x_2^2 + 22.69x_2 - 299.25$\n",
    "\n",
    "$-1.065x_1^2 + 34.52x_1 + 0.138x_2^2 + 2.61x_2 - 491.78 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe537c02cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red is people who dont buy a tv and blue is people who do\n"
     ]
    }
   ],
   "source": [
    "# Part B\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "%matplotlib inline\n",
    "\n",
    "def covariance(cov_x_2, cov_y_2, corr):\n",
    "    return np.array([[cov_x_2, corr * np.sqrt(cov_y_2)], [corr * np.sqrt(cov_x_2), cov_y_2]])\n",
    "\n",
    "def boundary_1(x):\n",
    "    return 0.00724638*(3.87298*math.sqrt(9798*x*x - 317584*x + 4.63791e6) - 1305)\n",
    "def boundary_2(x):\n",
    "    return 0.00724638*(-3.87298*math.sqrt(9798*x*x - 317584*x + 4.63791e6) - 1305)\n",
    "\n",
    "mean_c0_x = np.array([15,40])\n",
    "mean_c1_x = np.array([8.5,25])\n",
    "cov_c0_x = covariance(1., 4., 0.)\n",
    "cov_c1_x = covariance(9., 4., 0.)\n",
    "c0_x, c0_y = gauss_1 = np.random.multivariate_normal(mean_c0_x, cov_c0_x, size=1000).T\n",
    "c1_x, c1_y = gauss_2 = np.random.multivariate_normal(mean_c1_x, cov_c1_x, size=1000).T\n",
    "#sns.kdeplot(c0_x, c0_y)\n",
    "plt.scatter(c0_x,c0_y, c='red', s=5.0)\n",
    "plt.scatter(c1_x, c1_y, c='blue', s=5.0)\n",
    "X = np.linspace(0,20,2000)\n",
    "y_1 = []\n",
    "y_2 = []\n",
    "for x in X:\n",
    "    y_1.append(boundary_1(x))\n",
    "    y_2.append(boundary_2(x))\n",
    "plt.plot(X, y_1)\n",
    "plt.plot(X, y_2)\n",
    "plt.xlabel(\"salary\")\n",
    "plt.ylabel(\"age\")\n",
    "plt.show()\n",
    "print(\"red is people who dont buy a tv and blue is people who do\")\n",
    "#sns.kdeplot(c1_x, c1_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional math to derive decision boundary\n",
    "\n",
    "<img src=\"6_math.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
