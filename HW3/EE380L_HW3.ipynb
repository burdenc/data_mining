{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 80</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, March 20th, submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - Gradient Descent (5+12+3 = 20pts)\n",
    "\n",
    "In this question you will implement vanilla SGD and 2 adaptive gradient update techniques called Adagrad and Adadelta. In addition, you will also implement ridge regularization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Using stochastic gradient descent, derive the coefficent updates for all 4 coefficients of the model: $$ y = w_0 + w_1*x_1 + w_2*x_1*x_1 + w_3*x_2*x_2 + w_4*x_2 $$ Hint: start from the cost function (Assume sum of squared error). If you write the math by hand, submit that as a separate file and make a reference to it in your notebook or include the image in your notebook.\n",
    "2. Write Python code for an SGD solution to the non-linear model $$ y = w_0 + w_1*x_1 + w_2*x_1*x_1 + w_3*x_2*x_2 + w_4*x_2 $$ Try to format similarly to scikit-learn's models. Your Python class should take as input the learning_rate, regularization_constant and number of epochs. The fit method must take as input X,y and a choice of update_rule as 'SGD', 'adagrad' or 'adadelta' (Notes on implementation below). The _predict_ method takes an X value (optionally, an array of values). Use your new gradient descent regression to predict the data given in 'samples.csv', for 20 epochs (this may have to increase for adadelta), using learning rates: [.0001, .001, .01, 0.1, 1, 10, 100] and regularization constants in the range: [0,10,100] . Plot MSE and the $w$ parameters as a function of epoch count (20 epochs) for the best combination of learning_rate and regularization for both SGD, Adagrad and Adadelta. ie you should have a plot of MSE and parameter updates for SGD, adagrad and adadelta. Report the MSE at the end of 10 epochs for all 3.\n",
    "3. Based on the experiments, answer the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Among SGD and Adagrad, which one allows for larger initial setting of the learning_rate? Why?\n",
    "2. Mention one benefit and one drawback of Adadelta over Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes on Adagrad/Adadelta\n",
    "Adagrad (http://ruder.io/optimizing-gradient-descent/) differs from vanilla SGD in that the learning rate of each weight changes over updates. A cache is maintained that holds the sum of squares of all gradients upto the current update. The learning_rate is divided by the cache, resulting in a different learning rate for each weight. A consequence of this update rule is that weights that have already seen large gradients (made large jumps) make smaller updates in subsequent iterations.\n",
    "Specifically, the steps can be listed as below:\n",
    "1. cache = cache + (gradients^2)\n",
    "2. weights = weights + ((learning_rate)/sqrt(cache+1e-6))*gradients\n",
    "\n",
    "The key difference between Adadelta and Adagrad is that the former uses a weighted sum for each update of the cache. Also, you will not need to use a learning rate to make Adadelta work. In addition to the link above, the paper on Adadelta is a great resource to implement it correctly (https://arxiv.org/pdf/1212.5701.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) Notes on Training with Gradient Descent\n",
    "1. Compute error: This consists of a prediction error and a regularization term. From an implementation perspective, this is a function that takes as input the truth, prediction and regularization hyperparameter and returns an error\n",
    "2. Compute gradients: Take a derivative of the error in terms of the weights. This can be modelled as a function that takes as input the error and features and returns the gradients for each weight\n",
    "3. Update weights: Weight updates can be done using vanilla SGD or adaptive techniques. The update function takes as inputs the gradient and hyperparameters and returns the new weights."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "<img src=\"1_math.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "MSE:  15.317324833389403  Epoch_num:  0\n",
      "MSE:  1.0628220413179281  Epoch_num:  1\n",
      "MSE:  0.47092531589814496  Epoch_num:  2\n",
      "MSE:  0.20246977833123325  Epoch_num:  3\n",
      "MSE:  0.1182952969642734  Epoch_num:  4\n",
      "MSE:  0.09871997228173743  Epoch_num:  5\n",
      "MSE:  0.09153824905393862  Epoch_num:  6\n",
      "MSE:  0.08643653835862125  Epoch_num:  7\n",
      "MSE:  0.08186801454748122  Epoch_num:  8\n",
      "MSE:  0.07753377378630909  Epoch_num:  9\n",
      "MSE:  0.07335596403155704  Epoch_num:  10\n",
      "MSE:  0.06931027425560873  Epoch_num:  11\n",
      "MSE:  0.06538849287011755  Epoch_num:  12\n",
      "MSE:  0.061587710912614696  Epoch_num:  13\n",
      "MSE:  0.05790678274586159  Epoch_num:  14\n",
      "MSE:  0.05434512722909445  Epoch_num:  15\n",
      "MSE:  0.05090232578351474  Epoch_num:  16\n",
      "MSE:  0.04757799054815301  Epoch_num:  17\n",
      "MSE:  0.044371721646949525  Epoch_num:  18\n",
      "MSE:  0.04128309289475771  Epoch_num:  19\n",
      "MSE:  0.03831164634692477  Epoch_num:  20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, learning_rate, regularization_constant, epochs):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization_constant = regularization_constant\n",
    "        self.epochs = epochs\n",
    "        self.plot_weights = []\n",
    "        self.plot_errors = []\n",
    "        self.plot_epoch = []\n",
    "        self.epoch_num = 0\n",
    "        self.weights = np.random.random_sample(5,)\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        self.update_mode = 0\n",
    "        self.cache = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.cache = np.array(self.cache)\n",
    "        self.gradient_ms = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.gradient_ms = np.array(self.gradient_ms)\n",
    "        self.parameter_ms = [[0.0],[0.0],[0.0],[0.0],[0.0]]\n",
    "        self.parameter_ms = np.array(self.parameter_ms)\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y, update_rule=\"SGD\"):\n",
    "        if(update_rule == 'adagrad'):\n",
    "            self.update_mode = 1\n",
    "        elif(update_rule == 'adadelta'):\n",
    "            self.update_mode = 2\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.evaluate_model()\n",
    "        self.epoch_num = self.epoch_num + 1\n",
    "        while (self.epoch_num <= self.epochs):\n",
    "            for index in range(len(X)):\n",
    "                truth = y[index]\n",
    "                features = X[index]\n",
    "                #print(\"This is truth: \", truth)\n",
    "                #print(\"This is features: \", features)\n",
    "                prediction = self.predict_one(features)\n",
    "                #print(\"This is prediction: \", prediction)\n",
    "                error = self.compute_error(truth,prediction)\n",
    "                gradients = self.compute_gradients(error, features)\n",
    "                self.update_weights(gradients)\n",
    "            self.evaluate_model()\n",
    "            self.epoch_num = self.epoch_num + 1\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        predictions = self.predict_all(X)\n",
    "        mse = mean_squared_error(y,predictions)\n",
    "        self.plot_errors.append(mse)\n",
    "        self.plot_weights.append(self.weights) #may want to do deep copy here\n",
    "        self.plot_epoch.append(self.epoch_num)\n",
    "        print(\"MSE: \", mse, \" Epoch_num: \", self.epoch_num)\n",
    "    def predict_all(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            predictions.append(self.predict_one(x))\n",
    "        return predictions\n",
    "            \n",
    "    def predict_one(self, X):\n",
    "        return self.weights[0] + X[0]*self.weights[1] + X[0]*X[0]*self.weights[2] \\\n",
    "            + X[1]*X[1]*self.weights[3] + X[1]*self.weights[4] \n",
    "        \n",
    "    def compute_error(self, truth, prediction):\n",
    "        return prediction - truth\n",
    "    \n",
    "    def compute_gradients(self, error, features):\n",
    "        gradients = []\n",
    "        gradients.append(2*error + self.regularization_constant*self.weights[0])\n",
    "        gradients.append(2*features[0]*error + self.regularization_constant*self.weights[1])\n",
    "        gradients.append(2*features[0]*features[0]*error + self.regularization_constant*self.weights[2])\n",
    "        gradients.append(2*features[1]*features[1]*error + self.regularization_constant*self.weights[3])\n",
    "        gradients.append(2*features[1]*error + self.regularization_constant*self.weights[4])\n",
    "        #print(\"These are gradients: \", gradients)\n",
    "        return gradients\n",
    "    \n",
    "    def update_weights(self, gradients):\n",
    "        #print(\"Beginning weights: \", self.weights)\n",
    "        if(self.update_mode == 0):\n",
    "            self.weights[0] = self.weights[0] - self.learning_rate*gradients[0] \n",
    "            self.weights[1] = self.weights[1] - self.learning_rate*gradients[1]\n",
    "            self.weights[2] = self.weights[2] - self.learning_rate*gradients[2]\n",
    "            self.weights[3] = self.weights[3] - self.learning_rate*gradients[3]\n",
    "            self.weights[4] = self.weights[4] - self.learning_rate*gradients[4]\n",
    "        if(self.update_mode == 1):\n",
    "            gradients_squared = np.multiply(gradients,gradients)\n",
    "            #print(\"This is gradients_squared \", gradients_squared)\n",
    "            #print(\"This  was self.cache \", self.cache)\n",
    "            self.cache = np.add(self.cache, gradients_squared)\n",
    "            #print(\"This is now self.cache \", self.cache)\n",
    "            self.weights[0] = self.weights[0] - self.learning_rate*gradients[0]/math.sqrt(self.cache[0] + 1e-6)\n",
    "            self.weights[1] = self.weights[1] - self.learning_rate*gradients[1]/math.sqrt(self.cache[1] + 1e-6)\n",
    "            self.weights[2] = self.weights[2] - self.learning_rate*gradients[2]/math.sqrt(self.cache[2] + 1e-6)\n",
    "            self.weights[3] = self.weights[3] - self.learning_rate*gradients[3]/math.sqrt(self.cache[3] + 1e-6)\n",
    "            self.weights[4] = self.weights[4] - self.learning_rate*gradients[4]/math.sqrt(self.cache[4] + 1e-6)\n",
    "        if(self.update_mode == 2):\n",
    "            update_vals = [[0.0], [0.0], [0.0], [0.0], [0.0]]\n",
    "            update_vals = np.array(update_vals)\n",
    "            self.gradient_ms = np.add(np.multiply(0.9, self.gradient_ms), np.multiply(0.1,np.multiply(gradients,gradients)))\n",
    "            update_vals[0] = -1*gradients[0]*math.sqrt(self.parameter_ms[0] + 1e-6)/math.sqrt(self.gradient_ms[0] + 1e-6)\n",
    "            update_vals[1] = -1*gradients[1]*math.sqrt(self.parameter_ms[1] + 1e-6)/math.sqrt(self.gradient_ms[1] + 1e-6)\n",
    "            update_vals[2] = -1*gradients[2]*math.sqrt(self.parameter_ms[2] + 1e-6)/math.sqrt(self.gradient_ms[2] + 1e-6)\n",
    "            update_vals[3] = -1*gradients[3]*math.sqrt(self.parameter_ms[3] + 1e-6)/math.sqrt(self.gradient_ms[3] + 1e-6)\n",
    "            update_vals[4] = -1*gradients[4]*math.sqrt(self.parameter_ms[4] + 1e-6)/math.sqrt(self.gradient_ms[4] + 1e-6)\n",
    "            #update parameter rms\n",
    "            self.parameter_ms = np.add(np.multiply(0.9, self.parameter_ms), np.multiply(0.1,np.multiply(update_vals,update_vals)))\n",
    "            #print(\"gradients: \", gradients)\n",
    "            #print(\"update_vals: \", update_vals)\n",
    "            #print(\"gradient_ms: \", self.gradient_ms)\n",
    "            #print(\"parameter_ms: \", self.parameter_ms)\n",
    "            self.weights[0] = self.weights[0] + update_vals[0]\n",
    "            self.weights[1] = self.weights[1] + update_vals[1]\n",
    "            self.weights[2] = self.weights[2] + update_vals[2]\n",
    "            self.weights[3] = self.weights[3] + update_vals[3]\n",
    "            self.weights[4] = self.weights[4] + update_vals[4]\n",
    "        #print(\"Ending weights: \", self.weights)\n",
    "    \n",
    "    \n",
    "print (\"Hello\")\n",
    "df = pd.read_csv('samples.csv')\n",
    "df = df.dropna()\n",
    "X = df[[u'x_1',u'x_2']]\n",
    "X = X.as_matrix()\n",
    "X = X.astype('float')\n",
    "y = df[[u'y']]\n",
    "y = y.as_matrix()\n",
    "y = np.array(y)\n",
    "y = y.astype('float')\n",
    "sgd = SGD(0.1,0,20)\n",
    "sgd.fit(X,y,\"adadelta\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2. (6+4=10 pts) Tensor Playground\n",
    "Visit http://playground.tensorflow.org for this problem\n",
    "\n",
    "A. From the far right, select \"Regression\" as the problem type, and select the 2nd of the two data sets ( the right one ).  \n",
    "\n",
    "   i) What sort of test / training loss do you get if you run it for 200 epochs with the following learning rates: .3, .01 and .003 ?  What if you run it for 1000 epochs with these learning rates?  Leave all other values at their defaults ( test/training ratio 50%, Noise 0, Batch Size 10, using Tanh activation function, and No Regularization )\n",
    "   \n",
    "   ii) Keeping learning rate at .3, Activation at Tanh, with all others at their defaults, and running for 200 epochs.  \n",
    "     What sort of test/train loss can you achieve using only 1 neuron in the first hidden layer.  What about for 2,3 or 8 neurons?  Provide screen shots of output layer and comment on how the different output fits look and compare with one another.\n",
    "\n",
    "   iii)Now keeping learning rate at .03 with all others at their defaults, and running for 200 epochs.  \n",
    "       Compare how the activation functions affect the ouput ( ReLU, Sigmoid, Tanh, Linear ). Provide screen shots of output results and comment.\n",
    "\n",
    "\n",
    "B. Neural Nets can fit anything.  Now reset to the initial defaults, and select \"Classification\" as the problem type, and from the Data section, select the bottom right \"Spriral\" data set.  With the idea of trying to minimize training/testing error, provide solutions to the problem for the following 2 scenarios.  i) Using just the first 2 inputs ( as per default ) and ii) Using all 7 of the inputs.  You may use as many layers as you want, whatever activation, however man neurons.  Provide screen shots which show your full network, output and parameters. Briefly justify your decisions, and comment on difficulties/tradeoffs, what helps/what doesn't,etc. \n",
    "\n",
    "## ANSWER\n",
    "\n",
    "### i\n",
    "200 Epochs\n",
    "0.3 - test loss: 0.020, training loss: 0.016\n",
    "\n",
    "0.01 - test loss: .04, training loss: 0.03\n",
    "\n",
    "0.003 - test loss: 0.046, training loss: 0.036\n",
    "\n",
    "1000 Epochs\n",
    "0.3 - test loss: 0.012, training loss: 0.009\n",
    "\n",
    "0.01 - test loss: 0.019, training loss: 0.016\n",
    "\n",
    "0.003 - test loss: 0.024, training loss: 0.019\n",
    "\n",
    "### ii\n",
    "1 Neuron in 1st Hidden Layer\n",
    "\n",
    "test: 0.041, training: 0.038\n",
    "\n",
    "We can see that with just one neuron in the hidden layer the model lacks power in order to accurately predict this dataset. This is because the limited number of weights can not represent this function. \n",
    "<img src=\"2_1.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Neurons in Hidden Layer\n",
    "\n",
    "test: 0.029, training: 0.027\n",
    "\n",
    "As we add more features the models complexity increases and so does the power to represent more complicated functions. As we increase the number of weights our power increases and we can represent more complicated functions.\n",
    "<img src=\"2_2.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 neurons in Hidden Layer\n",
    "\n",
    "test: 0.022, training: 0.020\n",
    "\n",
    "This was a very big improvement, now we start to see the model correctly identifying the 3 very blue and 3 very orange areas. \n",
    "\n",
    "<img src=\"2_3.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 neurons in Hidden Layer\n",
    "\n",
    "test: 0.016, training: 0.015\n",
    "\n",
    "The model was already doing pretty well, which is why we do not see as large an improvement from 3 neurons to 8 neurons as from 2 to 3. We can see from this lowered benefit that we are near or already passed the threshold of the most powerful network which we need. It is very possible with increased complexity we may see no benefit at all because of overfitting.\n",
    "<img src=\"2_4.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iii\n",
    "\n",
    "tanh, test loss: 0.022, training loss 0.020\n",
    "\n",
    "tanh does the best of any function for this problem. We see it does the best at generalizing the data as well in that the boundaries look more natural. \n",
    "<img src=\"2_5.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RelU, test loss: 0.027, training loss 0.027\n",
    "\n",
    "RelU is very powerful because of its ability to not saturate. However, we also have the problem of neurons dying because of them falling into the less than 0 region of the RelU. \n",
    "\n",
    "<img src=\"2_6.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid, test loss: 0.041, training loss 0.044\n",
    "\n",
    "Sigmoid really struggles with this data set. I believed simgoid was supposed to be quite good, however it looks like it really struggles. This is possibly because of it not being zero centered which restricts later nodes in the model because no negative values can come out of the non linearity. \n",
    "\n",
    "<img src=\"2_7.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear, test loss: 0.040, training loss 0.044\n",
    "\n",
    "It is clear to see why linear would struggle with this problem given the data we are trying to model is clearly non linear.\n",
    "\n",
    "<img src=\"2_8.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b part i\n",
    "For this problem I knew I was going to need a deep and complex network in order to get a good solution. A lot of weights would be needed in order to turn just 2 features into this output. I also felt that depth would be good in order to generalize the shape of the spiral. Because I figured I would need a large and deep network, I though RelU would be better to help with the weight decay problem when using the tanh function. However, it did take me quite a few tries when running this network to get a really good answer because the space in which to get lost in is so much larger with this complicated of a network. \n",
    "<img src=\"2_9.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii\n",
    "For this problem I chose to go with a smaller and less deep network. Because of the inclusion of the other features, there are now non linear functions which can really help when trying to predict something complicated like a spiral. The two sin functions are very beneficial because of their circular nature too. Because of these additional features, I felt less neurons would be needed. Because less neurons were needed and the depth of the network was not too large, I believed a tanh function would do quite well. I did have to run this one a few times to get a really good answer, but not nearly as many times as the network in part i.\n",
    "<img src=\"2_10.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Multi-layer Perceptron Regressor (15 points)\n",
    "\n",
    "In this question, you will explore the application of Multi-layer Perceptron (MLP) regression using sklearn package in Python. We will use the OpenCL gemm kernel performance prediction dataset for this problem https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance.\n",
    "\n",
    "Following code will pre-process the data and split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 42** and **test_size = 0.33**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((161872, 14), (161872,), (79728, 14), (79728,))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline\n",
    "\n",
    "data = pd.read_csv('sgemm_product.csv') \n",
    "data['target'] = 0.25*(data['Run1 (ms)'] + data['Run2 (ms)']+ data['Run3 (ms)'] + data['Run4 (ms)'])\n",
    "y = data['target']\n",
    "X = data.drop(['target','Run1 (ms)', 'Run2 (ms)', 'Run3 (ms)', 'Run4 (ms)'], axis = 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=42)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to use in this problem is [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Instead of fitting a model on original data, use StandardScaler to make each feature centered ([Example](http://scikit-learn.org/stable/auto_examples/applications/plot_prediction_latency.html#sphx-glr-auto-examples-applications-plot-prediction-latency-py)). Whenever you have training and test data, fit a scaler on training data and use this scaler on test data. Here, scale only features (independent variables), not target variable y.\n",
    "\n",
    "Use [sklearn.neural_nework.MLPRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor) to do a 5-fold cross validation using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). The cross validation must be performed on the **training data**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Use following parameter settings for MLPRegressor:\n",
    "\n",
    "    activation = 'tanh', solver = 'sgd', learning_rate='constant', random_state=42,\n",
    "    batch_size=5000, learning_rate_init = 0.005\n",
    "    \n",
    "Now, consider the following settings for the number of hidden units:\n",
    "    \n",
    "        (2,), (10,), (20,), (30,), (40,)\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model (5pts)\n",
    "      \n",
    "2) Now, using the same number of hidden units used in part 1), train MLPRegressor models on whole training data and report RMSE score for both Train and Test set (Again, use StandardScaler). Which model works the best? Briefly analyze the result in terms of the number of hidden units. (3pts)\n",
    "\n",
    "3) MLPRegressor has a built-in attribute *loss\\_curve\\_* which returns the loss at each epoch (misleadingly referred to as \"iteration\" in scikit documentation, though they use epoch in the actual code!). For example, if your model is named as *my_model* you can call it as *my\\_model.loss\\_curve\\_* ([example](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html#sphx-glr-auto-examples-neural-networks-plot-mlp-training-curves-py)). Plot three curves for model (a), (b), and (c) in one figure, where *X-axis* is epoch  number and *Y-axis* is squared root of *loss\\_curve\\_* value. (2pts)\n",
    "\n",
    "4) Activation Layer: Use the tanh and relu activations for the following fixed parameters to train your model:\n",
    "    \n",
    "    solver = 'sgd', random_state=42,\n",
    "    batch_size=5000, hidden_layer_sizes = (2,)\n",
    "\n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for each model. (2pts)\n",
    "   \n",
    "5) Optimization technique: The sklearn API performs a momentum update when using SGD. To understand the difference in performance of vanilla SGD and SGD with momentum, set momentum to 0 with SGD. Here, we compare the performance of vanilla sgd against adam solvers. Since both of these use different optimization subroutines, the learning rate must also be varied. In this question, vary the learning rate through these values [1, 0.1, 0.01, 0.001]. Keep the following fixed parameters to train your model:\n",
    "\n",
    "    activation = 'tanh', random_state=42,\n",
    "    batch_size=5000, hidden_layer_sizes = (2,), momentum = 0\n",
    "    \n",
    "   Report the average Root Mean Squared Error (RMSE) value based on your 5-fold cross validation for the best learning rate for each model. (3pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "### 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Multi-level Model in Python (20 points)\n",
    "In this problem, you will explore multi-level model using a python package [PyMC3](https://pymc-devs.github.io/pymc3/index.html#). The dataset \"brew_by_year.csv\" will be used in this question.  This dataset contains fields:\n",
    "\n",
    "- `state`\n",
    "- `state_id`\n",
    "- `brew`\n",
    "- `year`\n",
    "\n",
    "Where 'brew' is the number of breweries in the state that year, and `state_id` is a unique ID for that state.\n",
    "\n",
    "(a)  (2 pts) Plot the relationship between number of breweries and year, and draw a linearly regressed line ignoring the ID variable.\n",
    "\n",
    "(b)  (2 pts) Plot the relationship between number of breweries and year, but this time, fit a different linear regression for each state.\n",
    "\n",
    "(c)  (2 + 4 + 6 pts) Divide the dataset into training and test sets.  The training set contains the first 6 years of the measurements, and the test set contains the rest of the measurements. \n",
    "Build three different linear models:\n",
    "- Global model:  a linear model using `brew` as the dependent variable and `year` as the independent variable. Pool all data and estimate one common regression model to assess the influence of the passage of time across the total number of breweries.\n",
    "- Local model:  a different linear model for each state i.e., 51 different linear regressions. We are interested in whether different states actually follow separate regression models.\n",
    "- Multilevel model:  Use the [PyMC3](http://pymc-devs.github.io/pymc3/notebooks/GLM-hierarchical.html#Partial-pooling:-Hierarchical-Regression-aka,-the-best-of-both-worlds) package to fit a multilevel model specified as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{brew}_{it} &= \\beta_{0i} + \\beta_{1i} \\text{year}_{it} + \\epsilon_{it}\\\\\n",
    "\\beta_{0i} &= \\beta_{00}  + \\eta_{0i} \\\\\n",
    "\\beta_{1i} &= \\beta_{10} + \\eta_{1i} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Predict the number of breweries for the next 2 years, and calculate the mean squared errors from the three models.\n",
    "\n",
    "(d)  (2 pts) Briefly state what do $\\beta_{00}$ and $\\beta_{10}$ mean in this multilevel model.\n",
    "\n",
    "(e) (2 pt) Visually assess from the trace plot whether the MLM specified in the problem is a reasonable choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('brew_by_year.csv', index_col=0)\n",
    "data['brew'] = (data['brew'] - np.mean(data['brew'])) / np.std(data['brew'])\n",
    "\n",
    "# Specify the hyper-parameter of Multilevel model:\n",
    "beta_00 = pm.Normal('mu_beta0', mu=0., sd=10)\n",
    "eta_0i = pm.HalfCauchy('eta_0', 5)\n",
    "beta_10 = pm.Normal('mu_beta1', mu=0., sd=10)\n",
    "eta_1i = pm.HalfCauchy('eta1', 5)\n",
    "    \n",
    "# Intercept and Slope\n",
    "beta_0i = pm.Normal('beta0', mu=beta_00, sd=eta_0i, shape=len(train_data.state_id.unique()))\n",
    "beta_1i = pm.Normal('beta1', mu=beta_10, sd=eta_1i, shape=len(train_data.state_id.unique()))    \n",
    "# Model error\n",
    "eps = pm.HalfCauchy('eps', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Decision Tree using Python (5 pts)\n",
    "In this problem, you will model the data using decision trees to perform a classification task on the energy dataset provided. The dataset has been preprocessed. Using the class tree.DecisionTreeClassifier (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.tree), build two different trees with a maximum depth of two using the split criteria (i) Gini and (ii) Entropy. Use entire data to build trees. Plot the two trees. If your classifier object is called clf, use the following commands to save the generated tree as a '.dot' file that can be used to visualize the tree using Webgraphviz: http://www.webgraphviz.com/\n",
    "\n",
    "Hint: see  http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from sklearn import tree\n",
    "from sklearn.externals.six import StringIO\n",
    "with open(\"decision_tree_gini.dot\", 'w') as f:\n",
    "    f = tree.export_graphviz(clf, out_file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy paste the contents of your '.dot' file into the text window on the website to visualize the trees. At which node(s) do they differ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('energydata_complete.csv') \n",
    "\n",
    "y = data['Appliances']\n",
    "X = data.drop(['date','Appliances', 'rv1', 'rv2'], axis = 1)\n",
    "\n",
    "\n",
    "from dateutil import parser\n",
    "timeData = np.array(data['date'])\n",
    "\n",
    "days = []\n",
    "hours = []\n",
    "for line in xrange(len(timeData)):\n",
    "    day = parser.parse(timeData[line]).weekday()\n",
    "    hour = parser.parse(timeData[line]).hour\n",
    "    days.append(day)\n",
    "    hours.append(hour)\n",
    "    \n",
    "X = pd.concat([X, pd.get_dummies(days), pd.get_dummies(hours)], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6. (2+5+3=10 pts) Bayes Decision Theory\n",
    "a. Explain what you understand by class-conditional likelihood, class priors, and posterior probability of a class given an input, and the relationship between them. Please define all symbols and equations used explicitly.\n",
    "( 2 points )\n",
    "\n",
    "b. Suppose you want to learn a binary classifier to predict whether or not a customer will buy a TV. The class label is 0 if the customer does not buy a TV and 1 if they do. For each customer, you are given two features, $x_1$ is the per hour salary and $x_2$ is the age. Assume that the class conditional distribution $p([x_1 , x_2]|C)$ is Gaussian. The mean salary and age of the people who do buy a TV is 15 and 40 respectively and that of those who don’t is 8.5 and 25. The first class of customers, $\\sigma_1$ = 1, $\\sigma_2 = 2$ and $\\rho = 0$. For the second class of customers, $\\sigma_1$ = 3, $\\sigma_2 = 2$ and $\\rho = 0$.(Refer HW1 on how to construct a covariance matrix with this information). Further, your sales data suggests that only 1 in 3 people actually bought a TV in the last few years. Mathematically derive the (optimal) Bayes decision boundary for this problem. (5 points)\n",
    "\n",
    "c. Now sample 1000 customers from each class (C = 0, 1) under the assumed distribution and the estimated parameters and plot their features. Additionally, plot the decision boundary you obtained in the part (b) on the same plot. (3 points)\n",
    "\n",
    "## ANSWER\n",
    "\n",
    "### a\n",
    "Class conditional likelihood function - $p([x_1,x_2]|C_i)$ \n",
    "This is likelihood of $x_1, x_2$ given the class. \n",
    "\n",
    "Class priors - $P(C_i)$\n",
    "This is the likelihood a given datapoint is apart of class $Ci$\n",
    "\n",
    "Posterior probability - $p(C_i|[x_1,x_2]) = \\frac{P(C_i)p([x_1,x_2]|C_i)}{p([x_1, x_2])}$\n",
    "This is the likelihood that for a given $x_1, x_2$ the datapoint is apart of class $C_i$. The optimal decision for each $x_1, x_2$ is to choose the $p(C_i|[x_1,x_2])$ which is greatest for that $x_1 and x_2$. \n",
    "\n",
    "Bayes decision boundary (boundaries) = $P(C_0| [x_1,x_2]) = P(C_1| [x_1,x_2])$\n",
    "We need to choose whichever posterior probability is higher, so whenever these quantities are equal, this will be our decision boundary. When solving for this boundary we do not even need to compare the denominators in the posterior probability because they will cancel out. Therefor this equation is really just $P(C_0)p([x_1,x_2]|C_0) = P(C_1)p([x_1,x_2]|C_1)$\n",
    "\n",
    "### b\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation} \\mu_0 = \\left[ \\begin{array}{cc} 15 \\\\ 40  \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_0 = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 4 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_0^{-1} = \\left[ \\begin{array}{cc} 1 & 0 \\\\ 0 & 0.25 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "$\\mid\\Sigma_0\\mid = 4$\n",
    "\n",
    "\\begin{equation} \\mu_1 = \\left[ \\begin{array}{cc} 8.5 \\\\ 25  \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_1 = \\left[ \\begin{array}{cc} 9 & 0 \\\\ 0 & 4 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "\\begin{equation} \\Sigma_1^{-1} = \\left[ \\begin{array}{cc} 0.111 & 0 \\\\ 0 & 0.25 \\end{array} \\right] \\end{equation}\n",
    "\n",
    "$\\mid\\Sigma_1\\mid = 36$\n",
    "\n",
    "$P(C_0) = 2/3$\n",
    "\n",
    "$P(C_1) = 1/3$\n",
    "\n",
    "$P(C_0)p([x_1,x_2]|C_0) = P(C_1)p([x_1,x_2]|C_1)$\n",
    "\n",
    "$2/3N(\\mu_0,\\Sigma_0) = 1/3N(\\mu_1,\\Sigma_1)$\n",
    "\n",
    "$\\ln(2/3) + \\ln(N(\\mu_0,\\Sigma_0)) = \\ln(1/3) + \\ln(N(\\mu_1,\\Sigma_1))$\n",
    "\n",
    "$\\ln(2/3)+-1/2(\\vec{x}-\\mu_0)^T\\Sigma^{-1}_0(\\vec{x}-\\mu_0)\\ln(2\\pi\\mid\\Sigma_0\\mid^{1/2})=\\ln(1/3) + -1/2(\\vec{x} - \\mu_1)^T\\Sigma^{-1}_1(\\vec{x}-\\mu_1)\\ln(2\\pi\\mid\\Sigma_1\\mid^{1/2})$\n",
    "\n",
    "A lot of reduction that I did by hand, included at end\n",
    "\n",
    "$-1.265x_1^2 + 37.95x_1 - 0.316x_2^2 + 25.3x_2 - 791.03 = -0.2x_1^2 + 3.43x_1 - 0.454x_2^2 + 22.69x_2 - 299.25$\n",
    "\n",
    "$-1.065x_1^2 + 34.52x_1 + 0.138x_2^2 + 2.61x_2 - 491.78 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe537c34080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part B\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set(style='ticks', palette='Set2')\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "%matplotlib inline\n",
    "\n",
    "def covariance(cov_x_2, cov_y_2, corr):\n",
    "    return np.array([[cov_x_2, corr * np.sqrt(cov_y_2)], [corr * np.sqrt(cov_x_2), cov_y_2]])\n",
    "\n",
    "def boundary_1(x):\n",
    "    return 0.00724638*(3.87298*math.sqrt(9798*x*x - 317584*x + 4.63791e6) - 1305)\n",
    "def boundary_2(x):\n",
    "    return 0.00724638*(-3.87298*math.sqrt(9798*x*x - 317584*x + 4.63791e6) - 1305)\n",
    "\n",
    "mean_c0_x = np.array([15,40])\n",
    "mean_c1_x = np.array([8.5,25])\n",
    "cov_c0_x = covariance(1., 4., 0.)\n",
    "cov_c1_x = covariance(9., 4., 0.)\n",
    "c0_x, c0_y = gauss_1 = np.random.multivariate_normal(mean_c0_x, cov_c0_x, size=1000).T\n",
    "c1_x, c1_y = gauss_2 = np.random.multivariate_normal(mean_c1_x, cov_c1_x, size=1000).T\n",
    "#sns.kdeplot(c0_x, c0_y)\n",
    "plt.scatter(c0_x,c0_y, c='red', s=5.0)\n",
    "plt.scatter(c1_x, c1_y, c='blue', s=5.0)\n",
    "X = np.linspace(0,20,2000)\n",
    "y_1 = []\n",
    "y_2 = []\n",
    "for x in X:\n",
    "    y_1.append(boundary_1(x))\n",
    "    y_2.append(boundary_2(x))\n",
    "plt.plot(X, y_1)\n",
    "plt.plot(X, y_2)\n",
    "plt.show()\n",
    "#sns.kdeplot(c1_x, c1_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional math to derive decision boundary\n",
    "\n",
    "<img src=\"6_math.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
