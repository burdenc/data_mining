{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 72</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, February 22nd, submitted via Canvas by 11:59 pm</p>\n",
    "### Cassidy Burden (cab5534), Ryan Meek (rjm3263)\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 1: Ridge and Lasso Regression (20 points)\n",
    "Use the following code to import the dow_jones dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dow_jones_index.data')\n",
    "df = df.dropna()\n",
    "X = df[[u'open', u'high', u'low', u'close',\n",
    "       u'volume', u'percent_change_price',\n",
    "       u'percent_change_volume_over_last_wk', u'previous_weeks_volume', u'next_weeks_close',\n",
    "       u'percent_change_next_weeks_price', u'days_to_next_dividend',\n",
    "       u'percent_return_next_dividend']]\n",
    "X = X.as_matrix()\n",
    "\n",
    "for feat in range(X.shape[1]):\n",
    "    try:\n",
    "        if \"$\" in X[0, feat]:\n",
    "            X[:,feat] = [i.split(\"$\")[1] for i in X[:,feat]]\n",
    "    except:\n",
    "        pass\n",
    "X = X.astype('float')\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "y = df[[u'next_weeks_open']]\n",
    "y = y.as_matrix()\n",
    "y = [i.split(\"$\")[1] for i in y[:, 0]]\n",
    "y = np.array(y)\n",
    "y = y.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You will reuse the same dataset that you used in HW1. In this HW, you will add additional second-order polynomial and interaction variables to the features matrix. Note that this code adds all combinations of the features with degree less than or equal to two; in practice one may introduce only a few based on domain knowledge or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The following code will split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 20** and **test_size = 0.33**.  Note: lambda is called alpha in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{6}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(6,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation on each model. The cross validation should happen on your training data using  average MSE as the scoring metric. (5pts)\n",
    "\n",
    "2) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? What do you observe about the coefficients returned from the ridge and lasso models? (5pts)\n",
    "\n",
    "\n",
    "3) What are the top 10 most important features for the lasso and ridge models for the hyperparameters that got the best MSE? (2pts)\n",
    "\n",
    "\n",
    "4) SKLearn can perform this cross validation for us and choose the best regularization hyper-parameter. It will automatically determine the range of $\\lambda$ s to search over. Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "    Report the best $\\lambda$ based on cross validation. (3 pts)\n",
    "    \n",
    "5) \n",
    "    i) Run Ridge, lasso with the best hyperparameters and report the MSE on test data. \n",
    "    ii) Run OLS on the training data with all the features and then with the features selected through lasso (features with non zero weights). Report the MSE on test data. (5 pts)\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 2: Bias-Variance Tradeoff (5 points)\n",
    "\n",
    "1. Describe the relationship between Ordinary Least Squares and Ridge Regression (the problem analyzed in the previous question) with reference to Bias-Variance trade-off.\n",
    "3. Which is a higher variance model, kNN with k = 1 or with k = 10?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 3 - Principal Component Analysis (15 points)\n",
    "\n",
    "Import the same dataset with the interaction variables as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dow_jones_index.data')\n",
    "df = df.dropna()\n",
    "X = df[[u'open', u'high', u'low', u'close',\n",
    "       u'volume', u'percent_change_price',\n",
    "       u'percent_change_volume_over_last_wk', u'previous_weeks_volume', u'next_weeks_close',\n",
    "       u'percent_change_next_weeks_price', u'days_to_next_dividend',\n",
    "       u'percent_return_next_dividend']]\n",
    "X = X.as_matrix()\n",
    "\n",
    "for feat in range(X.shape[1]):\n",
    "    try:\n",
    "        if \"$\" in X[0, feat]:\n",
    "            X[:,feat] = [i.split(\"$\")[1] for i in X[:,feat]]\n",
    "    except:\n",
    "        pass\n",
    "X = X.astype('float')\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "y = df[[u'next_weeks_open']]\n",
    "y = y.as_matrix()\n",
    "y = [i.split(\"$\")[1] for i in y[:, 0]]\n",
    "y = np.array(y)\n",
    "y = y.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "(a) Perform PCA using the sklearn [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package.  Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X_train matrix).  Refer to Figure 10.4 of JW for an example.  You may use the output attribute *explained variance ratio*. How many principal components (N1, N2, N3) are required to explain cumulative variance of 90%, 95%, and 99%, respectively? (6 pts)\n",
    "\n",
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. Fit PCA on X_train and predict on X_test. Compare the test MSE using N1, N2, N3 to using all features from PCA (6 pts)\n",
    "\n",
    "(d) Compare this to the MSE reported in Q1.For this dataset, would you use PCA/Lasso as a feature elimination technique based on: (3 pts)\n",
    "i) Interpretability of results\n",
    "ii) MSE value\n",
    "iii) Hyperparameter tuning?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 4 - Sampling (3+2=5pts)\n",
    "\n",
    "1 . Your firm is conducting a customer survey for a new product. You are tasked with estimating whether the product will be liked or disliked by the entire market. Unfortunately, you cannot reach all the customers. So you randomly sample 400 participants and ask them \"Will you like the product\" as a question. The responses are evenly split. But you attended a Data Mining course, so you know the estimate is not simply 50%. \n",
    "   \n",
    "What are the lower and upper bounds of probability corresponding to a confidence of 99% ($\\alpha = 0.99$)?\n",
    "\n",
    "\n",
    "2 . Your manager asks you to halve the probability range you reported in part 1. This can be accomplished in two different ways:\n",
    "\n",
    "    a) Reduce the confidence without going below 90%. \n",
    "    OR\n",
    "    b) Conduct a new survey to acquire more samples. In this case let us assume that the results are again evenly split.\n",
    "    \n",
    "Which option is viable? If a), what is the highest confidence you can use? And if b), how many samples do you need?\n",
    "\n",
    "## Answer\n",
    "\n",
    "### 1.\n",
    "\n",
    "$n = (p)(1-p)(\\frac{z_{\\alpha/2}}{\\epsilon})^2$\n",
    "\n",
    "$400 = (.5)(1-.5)(\\frac{2.58}{\\epsilon})^2$\n",
    "\n",
    "$\\frac{400}{.5 * .5} = (\\frac{2.58}{\\epsilon})^2$\n",
    "\n",
    "$\\pm\\sqrt{\\frac{400}{.5 * .5}} = \\frac{2.58}{\\epsilon}$\n",
    "\n",
    "$\\frac{2.58}{\\pm\\sqrt{\\frac{400}{.5 * .5}}} = \\epsilon$\n",
    "\n",
    "$\\epsilon = \\pm 0.0645$\n",
    "\n",
    "So our confidence interval is +/- 6.45%. So the lower bound of our probability is 43.55% and the upper bound is 56.45% (with 99% confidence).\n",
    "\n",
    "### 2. a\n",
    "\n",
    "Recalculating with our interval set to .03225 (changing our confidence):\n",
    "\n",
    "$400 = (.5)(1-.5)(\\frac{z}{.03225})^2$\n",
    "\n",
    "$z = \\pm\\sqrt{\\frac{400}{.25}} * .03225 = \\pm 1.29 \\sigma$\n",
    "\n",
    "$P(\\mu - z \\leq x \\leq \\mu + z) = P(.5 - 1.29\\sigma \\leq x \\leq .5 + 1.29\\sigma) =  0.80294$\n",
    "\n",
    "**No option a is not viable.**\n",
    "\n",
    "### 2. b\n",
    "\n",
    "Recalculating with our interval set to .03225 (changing our sample size):\n",
    "\n",
    "$n = (.5)(1-.5)(\\frac{2.58}{.03225})^2 = 1600$\n",
    "\n",
    "We need **1600** samples, which seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 5: Ridge and Lasso Regression (15 points)\n",
    "The ridge regression problem is characterized by the following loss function:\n",
    "$$\\mathcal{L}(\\beta) = ||X \\beta - y||_2^2 + \\lambda ||\\beta||^2_2$$\n",
    "\n",
    "The ridge regression problem is the case where the penalty term ($\\lambda ||\\beta||$) uses the squared l-2 norm ($\\lambda ||\\beta||^2$).\n",
    "\n",
    "a) Find the closed form solution to the ridge regresion problem\n",
    "$$\\underset{\\beta}{\\mathrm{argmin}} \\mathcal{L}(\\beta)$$\n",
    "where $\\mathcal{L}(\\beta)$ is the loss function above.\n",
    "\n",
    "b) Now given the SVD of $X$ as $U \\Sigma V^T$, express $X \\hat \\beta_{ridge}$ in terms of $U$, $V$ and $\\Sigma$\n",
    "\n",
    "c) Explain what the expression for $X \\hat \\beta_{ridge}$ derived above tells us about how the addition of the regularization term affects the solution. (Hint: think about $\\lambda ||\\beta||^2$ as a penalty term)\n",
    "\n",
    "# Answer\n",
    "\n",
    "### a\n",
    "\n",
    "$\\mathcal{L}(\\beta) = ||X \\beta - y||_2^2 + \\lambda ||\\beta||^2_2$\n",
    "\n",
    "$d[||f(X)||](X) = (\\widehat{f(X)})^\\intercal \\cdot d[X](X) => d[||X \\beta - y||](\\beta) = \\widehat{(X \\beta - y)}^\\intercal \\cdot X$\n",
    "\n",
    "$d[||X \\beta - y||_2^2](\\beta) = 2 ||X \\beta - y|| * \\widehat{(X \\beta - y)}^\\intercal \\cdot X = 2 (X \\beta - y)^\\intercal X$\n",
    "\n",
    "$d[\\mathcal{L}](\\beta) = 2 (X \\beta - y)^\\intercal X + 2 \\lambda \\beta^\\intercal = (2 (X \\beta - y)^\\intercal X + 2 \\lambda \\beta^\\intercal)^\\intercal = 2 X^\\intercal (X \\beta - y) + 2 \\lambda \\beta = 2 X^\\intercal X \\beta - X^\\intercal y + 2 \\lambda \\beta$\n",
    "\n",
    "$0 = 2 X^\\intercal X \\beta - 2 X^\\intercal y + 2 \\lambda \\beta$\n",
    "\n",
    "$X^\\intercal X \\beta + \\lambda \\beta = X^\\intercal y$\n",
    "\n",
    "$(X^\\intercal X + \\lambda I) \\beta = X^\\intercal y$\n",
    "\n",
    "$\\widehat{\\beta}_{ridge} = (X^\\intercal X + \\lambda I)^{-1} X^\\intercal y$\n",
    "\n",
    "### b\n",
    "\n",
    "$X X^\\intercal = (U \\Sigma V^\\intercal)^\\intercal (U \\Sigma V^\\intercal) = V \\Sigma^\\intercal U^\\intercal U \\Sigma V^\\intercal$\n",
    "\n",
    "(U is orthonormal, thus the middle term becomes identity)\n",
    "\n",
    "$X X^\\intercal = V \\Sigma^\\intercal \\Sigma V^\\intercal = V \\Sigma^2 V^\\intercal$\n",
    "\n",
    "$X \\widehat{\\beta}_{ridge} = X ((U \\Sigma V^\\intercal)^\\intercal (U \\Sigma V^\\intercal) - \\lambda I)^{-1} (U \\Sigma V^\\intercal) Y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 6 - Outliers and Huber Loss (12 pts)\n",
    "\n",
    "In this problem, we will use the same data set from the previous problem set to fit a linear model to the data using a Huber loss function rather than the l-2 norm usually used in OLS. sklearn has a nice API you can use: [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor)\n",
    "\n",
    "Below is a snippet from the previous problem set to help you get started. For this problem, the only independent variable will be the \"percent\\_change\\_price\" feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('dow_jones_index.data')\n",
    "stock = df[[\"percent_change_price\", \"next_weeks_open\"]]\n",
    "stock = stock.dropna()\n",
    "stock = stock.as_matrix()[1:]\n",
    "stock[:, 1] = [i.split(\"$\")[1] for i in stock[:, 1]]\n",
    "stock = stock.astype('float')\n",
    "stock[:, 0] = (stock[:, 0] - np.mean(stock[:, 0], axis=0))/np.std(stock[:, 0], axis=0)\n",
    "\n",
    "X = stock[:,:1]\n",
    "y = stock[:, 1]\n",
    "X_train = X[:400,]\n",
    "y_train = y[:400]\n",
    "\n",
    "X_test = X[400:,]\n",
    "y_test = y[400:,]\n",
    "\n",
    "clf_train = linear_model.LinearRegression()\n",
    "clf_train.fit(X_train, y_train)\n",
    "predictions_train = clf_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "print('MSE train: ', mse_train)\n",
    "\n",
    "predictions_test = clf_train.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('MSE test: ', mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "a) Now do the same for the Huber loss function and print the test and train MSE. Use a regularization coefficient of 0. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's artificially introduce some errors intro the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train_outliers = np.copy(y_train)\n",
    "y_train_outliers[0] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note that we would never actually do this in a real application, this is simply for demo purposes.\n",
    "\n",
    "b) Now create two models as before, but using the new y vector during training (one model using OLS and another using the Huber loss). Print the test and train MSE. Use a regularization coefficient of 0. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ANSWER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "c) Explain the difference in prediction performance of the two models in these two scenarios (one without significant outliers in the data, and the other with a single outlier). (2 pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "EE380L_HW2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
