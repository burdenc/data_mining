{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "\n",
    "# <p style=\"text-align: center;\">EE 380L: Data Mining</p>\n",
    "# <p style=\"text-align: center;\">Assignment 2</p>\n",
    "## <p style=\"text-align: center;\">Total points: 72</p>\n",
    "## <p style=\"text-align: center;\">Due: Tuesday, February 22nd, submitted via Canvas by 11:59 pm</p>\n",
    "### Cassidy Burden (cab5534), Ryan Meek (rjm3263)\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UTID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group.  \n",
    "\n",
    "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 1: Ridge and Lasso Regression (20 points)\n",
    "Use the following code to import the dow_jones dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dow_jones_index.data')\n",
    "df = df.dropna()\n",
    "X = df[[u'open', u'high', u'low', u'close',\n",
    "       u'volume', u'percent_change_price',\n",
    "       u'percent_change_volume_over_last_wk', u'previous_weeks_volume', u'next_weeks_close',\n",
    "       u'percent_change_next_weeks_price', u'days_to_next_dividend',\n",
    "       u'percent_return_next_dividend']]\n",
    "X = X.as_matrix()\n",
    "\n",
    "for feat in range(X.shape[1]):\n",
    "    try:\n",
    "        if \"$\" in X[0, feat]:\n",
    "            X[:,feat] = [i.split(\"$\")[1] for i in X[:,feat]]\n",
    "    except:\n",
    "        pass\n",
    "X = X.astype('float')\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "y = df[[u'next_weeks_open']]\n",
    "y = y.as_matrix()\n",
    "y = [i.split(\"$\")[1] for i in y[:, 0]]\n",
    "y = np.array(y)\n",
    "y = y.astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You will reuse the same dataset that you used in HW1. In this HW, you will add additional second-order polynomial and interaction variables to the features matrix. Note that this code adds all combinations of the features with degree less than or equal to two; in practice one may introduce only a few based on domain knowledge or experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this question, you will explore the application of Lasso and Ridge regression using sklearn package in Python. The following code will split the data into training and test set using [train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) with **random state 20** and **test_size = 0.33**.  Note: lambda is called alpha in sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1) Use sklearn.linear_model.Lasso and sklearn.linear_model.Ridge classes to do a [5-fold cross validation](http://scikit-learn.org/stable/auto_examples/exercises/plot_cv_diabetes.html#example-exercises-plot-cv-diabetes-py) using sklearn's [KFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold). For the sweep of the regularization parameter, we will look at a grid of values ranging from $\\lambda = 10^{6}$ to $\\lambda = 10^{-2}$. In Python, you can consider this range of values as follows:\n",
    "\n",
    "      import numpy as np\n",
    "\n",
    "      alphas =  10**np.linspace(6,-2,100)*0.5\n",
    "\n",
    "  Report the best chosen $\\lambda$ based on cross validation on each model. The cross validation should happen on your training data using  average MSE as the scoring metric. (5pts)\n",
    "\n",
    "2) Run ridge and lasso for all of the alphas specified above (on training data), and plot the coefficients learned for each of them - there should be one plot each for lasso and ridge, so a total of two plots; the plots for different features for a method should be on the same plot (e.g. Fig 6.6 of JW). What do you qualitatively observe when value of the regularization parameter is changed? What do you observe about the coefficients returned from the ridge and lasso models? (5pts)\n",
    "\n",
    "\n",
    "3) What are the top 10 most important features for the lasso and ridge models for the hyperparameters that got the best MSE? (2pts)\n",
    "\n",
    "\n",
    "4) SKLearn can perform this cross validation for us and choose the best regularization hyper-parameter. It will automatically determine the range of $\\lambda$ s to search over. Run lasso again with cross validation using [sklearn.linear_model.LassoCV](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html). Set the cross validation parameters as follows:\n",
    "\n",
    "    LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "\n",
    "    Report the best $\\lambda$ based on cross validation. (3 pts)\n",
    "    \n",
    "5) \n",
    "    i) Run Ridge, lasso with the best hyperparameters and report the MSE on test data. \n",
    "    ii) Run OLS on the training data with all the features and then with the features selected through lasso (features with non zero weights). Report the MSE on test data. (5 pts)\n",
    "\n",
    "## Answer\n",
    "\n",
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "alphas =  10**np.linspace(6,-2,100)*0.5\n",
    "kf = KFold(n_splits=5, random_state=20)\n",
    "\n",
    "lasso_max_alpha = (0, np.inf)\n",
    "ridge_max_alpha = (0, np.inf)\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso_error = 0\n",
    "    ridge_error = 0\n",
    "    \n",
    "    for train_index, test_index in kf.split(X_train):\n",
    "        X_kf_train, X_kf_test = X_train[train_index], X_train[test_index]\n",
    "        y_kf_train, y_kf_test = y_train[train_index], y_train[test_index]\n",
    "        \n",
    "        lasso = Lasso(alpha)\n",
    "        ridge = Ridge(alpha)\n",
    "        \n",
    "        lasso_fit = lasso.fit(X_kf_train, y_kf_train)\n",
    "        ridge_fit = ridge.fit(X_kf_train, y_kf_train)\n",
    "        \n",
    "        lasso_error += mean_squared_error(lasso_fit.predict(X_kf_test), y_kf_test)\n",
    "        ridge_error += mean_squared_error(ridge_fit.predict(X_kf_test), y_kf_test)\n",
    "    \n",
    "    lasso_error /= 5\n",
    "    ridge_error /= 5\n",
    "        \n",
    "    if (lasso_error < lasso_max_alpha[1]):\n",
    "        lasso_max_alpha = (alpha, lasso_error)\n",
    "    if (ridge_error < ridge_max_alpha[1]):\n",
    "        ridge_max_alpha = (alpha, ridge_error)\n",
    "\n",
    "print(\"Lasso best alpha is \", lasso_max_alpha[0], \" with MSE of \", lasso_max_alpha[1])\n",
    "print(\"Ridge best alpha is \", ridge_max_alpha[0], \" with MSE of \", ridge_max_alpha[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_lasso_coefs = []\n",
    "found_ridge_coefs = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha)\n",
    "    ridge = Ridge(alpha)\n",
    "    \n",
    "    lasso_fit = lasso.fit(X_train, y_train)\n",
    "    ridge_fit = ridge.fit(X_train, y_train)\n",
    "    \n",
    "    found_lasso_coefs.append(lasso_fit.coef_)\n",
    "    found_ridge_coefs.append(ridge_fit.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "plt.title(\"Lasso\")\n",
    "for alpha_coefs in np.array(found_lasso_coefs).T:\n",
    "    plt.ylabel('Coefficient Weight')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.plot(alphas, alpha_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Ridge\")\n",
    "for alpha_coefs in np.array(found_ridge_coefs).T:\n",
    "    plt.ylabel('Coefficient Weight')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Alpha')\n",
    "    plt.plot(alphas, alpha_coefs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lasso model prefers using much less terms, while the ridge model prefers small (but not quite zero) coefficients scattered about. At a glance it's much easier to tell which terms are important from the lasso model.\n",
    "### 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso best features:  [ 0  8  5 28  1 17  3  6 11 63]\n",
      "Ridge best features:  [ 8  0  3 46 49 53 16 18 32 34]\n"
     ]
    }
   ],
   "source": [
    "lasso_best_coef = found_lasso_coefs[int(np.where(alphas==lasso_max_alpha[0])[0])]\n",
    "ridge_best_coef = found_ridge_coefs[int(np.where(alphas==ridge_max_alpha[0])[0])]\n",
    "\n",
    "lasso_best_coef_idx = np.argsort(lasso_best_coef)\n",
    "ridge_best_coef_idx = np.argsort(ridge_best_coef)\n",
    "\n",
    "lasso_best_features = lasso_best_coef_idx[-10:][::-1]\n",
    "ridge_best_features = ridge_best_coef_idx[-10:][::-1]\n",
    "\n",
    "print(\"Lasso best features: \", lasso_best_features)\n",
    "print(\"Ridge best features: \", ridge_best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LassoCV found alpha:  0.046611216595171215\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "lasso_cv = LassoCV(alphas=None, cv=10, max_iter=10000)\n",
    "lasso_cv_fit = lasso_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"LassoCV found alpha: \", lasso_cv_fit.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso best alpha is  0.008737642000038414  with MSE of  0.15983136817357\n",
      "Ridge best alpha is  0.005  with MSE of  0.0019317330320313645\n",
      "OLS has MSE of  0.00025540890208644953\n",
      "OLS (Lasso Filtered) has MSE of  0.00017816277366227142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lasso = Lasso(lasso_max_alpha[0])\n",
    "lasso_fit = lasso.fit(X_train, y_train)\n",
    "lasso_mse = mean_squared_error(lasso_fit.predict(X_test), y_test)\n",
    "\n",
    "lasso = Ridge(ridge_max_alpha[0])\n",
    "ridge_fit = ridge.fit(X_train, y_train)\n",
    "ridge_mse = mean_squared_error(ridge_fit.predict(X_test), y_test)\n",
    "\n",
    "OLS = LinearRegression()\n",
    "OLS_fit = OLS.fit(X_train, y_train)\n",
    "OLS_MSE = mean_squared_error(OLS_fit.predict(X_test), y_test)\n",
    "\n",
    "lasso_best_coef_idx_nonzero = lasso_best_coef.nonzero()[0]\n",
    "X_train_filtered = X_train[:,lasso_best_coef_idx_nonzero]\n",
    "X_test_filtered = X_test[:,lasso_best_coef_idx_nonzero]\n",
    "OLS_lasso_filtered = OLS.fit(X_train_filtered, y_train)\n",
    "OLS_lasso_filtered_MSE = mean_squared_error(OLS_lasso_filtered.predict(X_test_filtered), y_test)\n",
    "\n",
    "print(\"Lasso best alpha is \", lasso_max_alpha[0], \" with MSE of \", lasso_mse)\n",
    "print(\"Ridge best alpha is \", ridge_max_alpha[0], \" with MSE of \", ridge_mse)\n",
    "print(\"OLS has MSE of \", OLS_MSE)\n",
    "print(\"OLS (Lasso Filtered) has MSE of \", OLS_lasso_filtered_MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 2: Bias-Variance Tradeoff (5 points)\n",
    "\n",
    "1. Describe the relationship between Ordinary Least Squares and Ridge Regression (the problem analyzed in the previous question) with reference to Bias-Variance trade-off.\n",
    "3. Which is a higher variance model, kNN with k = 1 or with k = 10?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares will have higher variance because the lack of a regularization term will cause the model to try and fit the data exactly. However, this solution can cause problems with overfitting which causes the variance to increase. However, because of this minimization of the errors, the bias for ordinary least squares will be less because the average of our solutions will approach the true value. When a regularization term is included, it is possible we will underfit the curve and cause our bias to increase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = 1 will have higher variance, the lower radius will cause the answers to vary more greatly. Because we're averaging together less samples, more variance will show in the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 3 - Principal Component Analysis (15 points)\n",
    "\n",
    "Import the same dataset with the interaction variables as in Q1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('dow_jones_index.data')\n",
    "df = df.dropna()\n",
    "X = df[[u'open', u'high', u'low', u'close',\n",
    "       u'volume', u'percent_change_price',\n",
    "       u'percent_change_volume_over_last_wk', u'previous_weeks_volume', u'next_weeks_close',\n",
    "       u'percent_change_next_weeks_price', u'days_to_next_dividend',\n",
    "       u'percent_return_next_dividend']]\n",
    "X = X.as_matrix()\n",
    "\n",
    "for feat in range(X.shape[1]):\n",
    "    try:\n",
    "        if \"$\" in X[0, feat]:\n",
    "            X[:,feat] = [i.split(\"$\")[1] for i in X[:,feat]]\n",
    "    except:\n",
    "        pass\n",
    "X = X.astype('float')\n",
    "X = (X - np.mean(X, axis=0))/np.std(X, axis=0)\n",
    "\n",
    "y = df[[u'next_weeks_open']]\n",
    "y = y.as_matrix()\n",
    "y = [i.split(\"$\")[1] for i in y[:, 0]]\n",
    "y = np.array(y)\n",
    "y = y.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = PolynomialFeatures(2, include_bias=False).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.33, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "(a) Perform PCA using the sklearn [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) package.  Create i) a scree plot depicting the proportion of variance and ii) a cumulative proportion of variance explained by the principal components of the data (X_train matrix).  Refer to Figure 10.4 of JW for an example.  You may use the output attribute *explained variance ratio*. How many principal components (N1, N2, N3) are required to explain cumulative variance of 90%, 95%, and 99%, respectively? (6 pts)\n",
    "\n",
    "(c) Fit an ordinary least squares linear regression using N1, N2, and N3 number of principal components, respectively. Fit PCA on X_train and predict on X_test. Compare the test MSE using N1, N2, N3 to using all features from PCA (6 pts)\n",
    "\n",
    "(d) Compare this to the MSE reported in Q1.For this dataset, would you use PCA/Lasso as a feature elimination technique based on: (3 pts)\n",
    "i) Interpretability of results\n",
    "ii) MSE value\n",
    "iii) Hyperparameter tuning?\n",
    "\n",
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff73071fe80>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff73071fbe0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "cumulative_var_ratio = []\n",
    "eigen_vector = []\n",
    "cur_ratio = 0\n",
    "num = 0\n",
    "for r in pca.explained_variance_ratio_:\n",
    "    cur_ratio = cur_ratio + r\n",
    "    num = num + 1\n",
    "    cumulative_var_ratio.append(cur_ratio)\n",
    "    eigen_vector.append(num)\n",
    "\n",
    "\n",
    "plt.plot(eigen_vector, pca.explained_variance_ratio_)\n",
    "plt.ylabel(\"Explained Variance Ratio\")\n",
    "plt.xlabel(\"Principle Component\")\n",
    "plt.show()\n",
    "plt.plot(eigen_vector, cumulative_var_ratio)\n",
    "plt.ylabel(\"Cumulative Explained Variance Ratio\")\n",
    "plt.xlabel(\"Principle Component\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The required principle components for 90%, 95%, 99%, are 12, 18, and 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error test N1 components:  145.46920237766864\n",
      "Mean squared error test N2 components:  27.725886102496414\n",
      "Mean squared error test N3 components:  3.1981565974283197\n"
     ]
    }
   ],
   "source": [
    "pca1 = PCA(n_components=12)\n",
    "X_train_trans0 = pca1.fit_transform(X_train)\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train_trans0, y_train)\n",
    "X_test_trans0 = pca1.transform(X_test)\n",
    "predictions_test = reg.predict(X_test_trans0)\n",
    "print(\"Mean squared error test N1 components: \" , mean_squared_error(y_test, predictions_test))\n",
    "pca2 = PCA(n_components=18)\n",
    "X_train_trans1 = pca2.fit_transform(X_train)\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train_trans1, y_train)\n",
    "X_test_trans1 = pca2.transform(X_test)\n",
    "predictions_test = reg.predict(X_test_trans1)\n",
    "print(\"Mean squared error test N2 components: \" , mean_squared_error(y_test, predictions_test))\n",
    "pca3 = PCA(n_components=29)\n",
    "X_train_trans2 = pca3.fit_transform(X_train)\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X_train_trans2, y_train)\n",
    "X_test_trans2 = pca3.transform(X_test)\n",
    "predictions_test = reg.predict(X_test_trans2)\n",
    "print(\"Mean squared error test N3 components: \" , mean_squared_error(y_test, predictions_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Compare this to the MSE reported in Q1.For this dataset, would you use PCA/Lasso as a feature elimination technique based on: (3 pts) i) Interpretability of results ii) MSE value iii) Hyperparameter tuning?\n",
    "### d\n",
    "\n",
    "The MSE for using all components of the PCA analysis is almost equal to the smallest MSE's (lasso filtered OLS: 0.00017816...) reported in Q1. \n",
    "### i) \n",
    "Interpretability of results is definitely more advantageous in Lasso because the features are simply the features in the original dataset rather than a linear combination of a number of features. \n",
    "### ii) \n",
    "The MSE value is very similar in the best case for both so I would likely call this about a tie, but depends on the hyperparameters set (number of components in PCA vs alpha in Lasso). \n",
    "### iii) \n",
    "Tuning the Hyperparameters is much easier in PCA because adding more components will always increase the amount of variance explained, which will likely decrease your MSE as opposed to Lasso there is the delicate balance of overfitting vs underfitting when setting alpha. While PCA can still have the issue of overfitting, it is much easier to set the number of componenets because you know the exact amount of variance explained  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 4 - Sampling (3+2=5pts)\n",
    "\n",
    "1 . Your firm is conducting a customer survey for a new product. You are tasked with estimating whether the product will be liked or disliked by the entire market. Unfortunately, you cannot reach all the customers. So you randomly sample 400 participants and ask them \"Will you like the product\" as a question. The responses are evenly split. But you attended a Data Mining course, so you know the estimate is not simply 50%. \n",
    "   \n",
    "What are the lower and upper bounds of probability corresponding to a confidence of 99% ($\\alpha = 0.99$)?\n",
    "\n",
    "\n",
    "2 . Your manager asks you to halve the probability range you reported in part 1. This can be accomplished in two different ways:\n",
    "\n",
    "    a) Reduce the confidence without going below 90%. \n",
    "    OR\n",
    "    b) Conduct a new survey to acquire more samples. In this case let us assume that the results are again evenly split.\n",
    "    \n",
    "Which option is viable? If a), what is the highest confidence you can use? And if b), how many samples do you need?\n",
    "\n",
    "## Answer\n",
    "\n",
    "### 1.\n",
    "\n",
    "$n = (p)(1-p)(\\frac{z_{\\alpha/2}}{\\epsilon})^2$\n",
    "\n",
    "$400 = (.5)(1-.5)(\\frac{2.58}{\\epsilon})^2$\n",
    "\n",
    "$\\frac{400}{.5 * .5} = (\\frac{2.58}{\\epsilon})^2$\n",
    "\n",
    "$\\pm\\sqrt{\\frac{400}{.5 * .5}} = \\frac{2.58}{\\epsilon}$\n",
    "\n",
    "$\\frac{2.58}{\\pm\\sqrt{\\frac{400}{.5 * .5}}} = \\epsilon$\n",
    "\n",
    "$\\epsilon = \\pm 0.0645$\n",
    "\n",
    "So our confidence interval is +/- 6.45%. So the lower bound of our probability is 43.55% and the upper bound is 56.45% (with 99% confidence).\n",
    "\n",
    "### 2. a\n",
    "\n",
    "Recalculating with our interval set to .03225 (changing our confidence):\n",
    "\n",
    "$400 = (.5)(1-.5)(\\frac{z}{.03225})^2$\n",
    "\n",
    "$z = \\pm\\sqrt{\\frac{400}{.25}} * .03225 = \\pm 1.29 \\sigma$\n",
    "\n",
    "$P(\\mu - z \\leq x \\leq \\mu + z) = P(.5 - 1.29\\sigma \\leq x \\leq .5 + 1.29\\sigma) =  0.80294$\n",
    "\n",
    "**No option a is not viable.**\n",
    "\n",
    "### 2. b\n",
    "\n",
    "Recalculating with our interval set to .03225 (changing our sample size):\n",
    "\n",
    "$n = (.5)(1-.5)(\\frac{2.58}{.03225})^2 = 1600$\n",
    "\n",
    "We need **1600** samples, which seems reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 5: Ridge and Lasso Regression (15 points)\n",
    "The ridge regression problem is characterized by the following loss function:\n",
    "$$\\mathcal{L}(\\beta) = ||X \\beta - y||_2^2 + \\lambda ||\\beta||^2_2$$\n",
    "\n",
    "The ridge regression problem is the case where the penalty term ($\\lambda ||\\beta||$) uses the squared l-2 norm ($\\lambda ||\\beta||^2$).\n",
    "\n",
    "a) Find the closed form solution to the ridge regresion problem\n",
    "$$\\underset{\\beta}{\\mathrm{argmin}} \\mathcal{L}(\\beta)$$\n",
    "where $\\mathcal{L}(\\beta)$ is the loss function above.\n",
    "\n",
    "b) Now given the SVD of $X$ as $U \\Sigma V^T$, express $X \\hat \\beta_{ridge}$ in terms of $U$, $V$ and $\\Sigma$\n",
    "\n",
    "c) Explain what the expression for $X \\hat \\beta_{ridge}$ derived above tells us about how the addition of the regularization term affects the solution. (Hint: think about $\\lambda ||\\beta||^2$ as a penalty term)\n",
    "\n",
    "# Answer\n",
    "\n",
    "### a\n",
    "\n",
    "$\\mathcal{L}(\\beta) = ||X \\beta - y||_2^2 + \\lambda ||\\beta||^2_2$\n",
    "\n",
    "$d[||f(X)||](X) = (\\widehat{f(X)})^\\intercal \\cdot d[X](X) => d[||X \\beta - y||](\\beta) = \\widehat{(X \\beta - y)}^\\intercal \\cdot X$\n",
    "\n",
    "$d[||X \\beta - y||_2^2](\\beta) = 2 ||X \\beta - y|| * \\widehat{(X \\beta - y)}^\\intercal \\cdot X = 2 (X \\beta - y)^\\intercal X$\n",
    "\n",
    "$d[\\mathcal{L}](\\beta) = 2 (X \\beta - y)^\\intercal X + 2 \\lambda \\beta^\\intercal = (2 (X \\beta - y)^\\intercal X + 2 \\lambda \\beta^\\intercal)^\\intercal = 2 X^\\intercal (X \\beta - y) + 2 \\lambda \\beta = 2 X^\\intercal X \\beta - X^\\intercal y + 2 \\lambda \\beta$\n",
    "\n",
    "$0 = 2 X^\\intercal X \\beta - 2 X^\\intercal y + 2 \\lambda \\beta$\n",
    "\n",
    "$X^\\intercal X \\beta + \\lambda \\beta = X^\\intercal y$\n",
    "\n",
    "$(X^\\intercal X + \\lambda I) \\beta = X^\\intercal y$\n",
    "\n",
    "$\\widehat{\\beta}_{ridge} = (X^\\intercal X + \\lambda I)^{-1} X^\\intercal y$\n",
    "\n",
    "### b\n",
    "\n",
    "$$X X^\\intercal = (U \\Sigma V^\\intercal)^\\intercal (U \\Sigma V^\\intercal) = V \\Sigma^\\intercal U^\\intercal U \\Sigma V^\\intercal$$\n",
    "\n",
    "$$X X^\\intercal = V \\Sigma^\\intercal \\Sigma V^\\intercal = V \\Sigma^2 V^\\intercal$$\n",
    "\n",
    "$$X \\widehat{\\beta}_{ridge} = (U \\Sigma V^\\intercal) (V \\Sigma^2 V^\\intercal + \\lambda I)^{-1} (V \\Sigma U^\\intercal) Y$$\n",
    "\n",
    "(The left V becomes and inverse on the right, the right V becomes and inverse transposed on the left)\n",
    "\n",
    "$$X \\widehat{\\beta}_{ridge} = U \\Sigma (V^\\intercal)^{-1} (V \\Sigma^2 V^\\intercal + \\lambda I)^{-1} (V)^{-1} \\Sigma U^\\intercal Y = U \\Sigma (V^\\intercal V \\Sigma^2 V^\\intercal V + \\lambda V^\\intercal V)^{-1} \\Sigma U^\\intercal Y$$\n",
    "\n",
    "$$X \\widehat{\\beta}_{ridge} = U \\Sigma (\\Sigma^2 + \\lambda I)^{-1} \\Sigma U^\\intercal Y$$\n",
    "\n",
    "### c\n",
    "\n",
    "The inner section can be expressed as:\n",
    "\n",
    "$$\\sum_{i=0}^{n} \\frac{d_i^2}{d_i^2 + \\lambda}$$\n",
    "\n",
    "This means as lambda gets bigger the components will be scaled closer to 0. Larger components are less affected by being divided by a constant lambda, especially with the squared term involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Question 6 - Outliers and Huber Loss (12 pts)\n",
    "\n",
    "In this problem, we will use the same data set from the previous problem set to fit a linear model to the data using a Huber loss function rather than the l-2 norm usually used in OLS. sklearn has a nice API you can use: [http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn-linear-model-huberregressor)\n",
    "\n",
    "Below is a snippet from the previous problem set to help you get started. For this problem, the only independent variable will be the \"percent\\_change\\_price\" feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train:  953.538042822908\n",
      "MSE test:  1192.1733687430108\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('dow_jones_index.data')\n",
    "stock = df[[\"percent_change_price\", \"next_weeks_open\"]]\n",
    "stock = stock.dropna()\n",
    "stock = stock.as_matrix()[1:]\n",
    "stock[:, 1] = [i.split(\"$\")[1] for i in stock[:, 1]]\n",
    "stock = stock.astype('float')\n",
    "stock[:, 0] = (stock[:, 0] - np.mean(stock[:, 0], axis=0))/np.std(stock[:, 0], axis=0)\n",
    "\n",
    "X = stock[:,:1]\n",
    "y = stock[:, 1]\n",
    "X_train = X[:400,]\n",
    "y_train = y[:400]\n",
    "\n",
    "X_test = X[400:,]\n",
    "y_test = y[400:,]\n",
    "\n",
    "clf_train = linear_model.LinearRegression()\n",
    "clf_train.fit(X_train, y_train)\n",
    "predictions_train = clf_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "print('MSE train: ', mse_train)\n",
    "\n",
    "predictions_test = clf_train.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('MSE test: ', mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "a) Now do the same for the Huber loss function and print the test and train MSE. Use a regularization coefficient of 0. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train:  969.0750882200402\n",
      "MSE test:  1235.9698312114529\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "df = pd.read_csv('dow_jones_index.data')\n",
    "stock = df[[\"percent_change_price\", \"next_weeks_open\"]]\n",
    "stock = stock.dropna()\n",
    "stock = stock.as_matrix()[1:]\n",
    "stock[:, 1] = [i.split(\"$\")[1] for i in stock[:, 1]]\n",
    "stock = stock.astype('float')\n",
    "stock[:, 0] = (stock[:, 0] - np.mean(stock[:, 0], axis=0))/np.std(stock[:, 0], axis=0)\n",
    "\n",
    "X = stock[:,:1]\n",
    "y = stock[:, 1]\n",
    "X_train = X[:400,]\n",
    "y_train = y[:400]\n",
    "\n",
    "X_test = X[400:,]\n",
    "y_test = y[400:,]\n",
    "\n",
    "clf_train = linear_model.HuberRegressor(alpha=0)\n",
    "clf_train.fit(X_train, y_train)\n",
    "predictions_train = clf_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train, predictions_train)\n",
    "print('MSE train: ', mse_train)\n",
    "\n",
    "predictions_test = clf_train.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('MSE test: ', mse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Now let's artificially introduce some errors intro the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train_outliers = np.copy(y_train)\n",
    "y_train_outliers[0] = 10000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Note that we would never actually do this in a real application, this is simply for demo purposes.\n",
    "\n",
    "b) Now create two models as before, but using the new y vector during training (one model using OLS and another using the Huber loss). Print the test and train MSE. Use a regularization coefficient of 0. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train Huber:  248826.75026848132\n",
      "MSE test Huber:  1232.5469165949833\n",
      "MSE train OLS:  246073.32477878802\n",
      "MSE test OLS:  3570.050639927662\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "y_train_outliers = np.copy(y_train)\n",
    "y_train_outliers[0] = 10000.0\n",
    "\n",
    "clf_train = linear_model.HuberRegressor(alpha=0)\n",
    "clf_train.fit(X_train, y_train_outliers)\n",
    "predictions_train = clf_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train_outliers, predictions_train)\n",
    "print('MSE train Huber: ', mse_train)\n",
    "\n",
    "predictions_test = clf_train.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('MSE test Huber: ', mse_test)\n",
    "\n",
    "\n",
    "ol_train = linear_model.LinearRegression()\n",
    "ol_train.fit(X_train, y_train_outliers)\n",
    "predictions_train = ol_train.predict(X_train)\n",
    "mse_train = mean_squared_error(y_train_outliers, predictions_train)\n",
    "print('MSE train OLS: ', mse_train)\n",
    "\n",
    "predictions_test = ol_train.predict(X_test)\n",
    "mse_test = mean_squared_error(y_test, predictions_test)\n",
    "print('MSE test OLS: ', mse_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "c) Explain the difference in prediction performance of the two models in these two scenarios (one without significant outliers in the data, and the other with a single outlier). (2 pts)\n",
    "\n",
    "#Answer\n",
    "\n",
    "Because the Huber loss is less sensitive to outliers (because of the linear increase in cost as opposed to quadratic increase in cost), the model computed will not shift as greatly towards the outlier point in training data. This is why the MSE will be worse for the training data in the Huber loss model. However, the MSE will be less with respect to the test data, because the Huber loss model will have shifted less to incorporate the outlier point.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "name": "EE380L_HW2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
